apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama4-scout-decode
  namespace: llm-d
  labels:
    app.kubernetes.io/name: llama4-scout-decode
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: llama4-scout-modelservice
    llm-d.ai/role: decode
spec:
  replicas: 3  # 3 instances across 3 nodes
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: llama4-scout-modelservice
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: llama4-scout-modelservice
        llm-d.ai/role: decode
    spec:
      nodeSelector:
        node-role.kubernetes.io/llm-inference: ""
      tolerations:
      - key: "llm-inference"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      serviceAccount: llama4-scout-service-account
      serviceAccountName: llama4-scout-service-account
      containers:
      - name: vllm
        image: ghcr.io/llm-d/llm-d:v0.2.0
        command: ["vllm", "serve", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic"]
        args:
        - --host
        - 0.0.0.0
        - --port
        - "8000"
        - --enable-prefix-caching
        - --prefix-caching-hash-algo
        - builtin
        - --block-size
        - "16"
        - --enforce-eager
        - --no-enable-chunked-prefill
        - --kv-transfer-config
        - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
        - --max-model-len
        - "4096"
        - --tensor-parallel-size
        - "1"
        - --gpu-memory-utilization
        - "0.85"
        - --swap-space
        - "16"
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: llm-d-hf-token
              key: HF_TOKEN
        - name: PYTHONHASHSEED
          value: "42"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: HF_HOME
          value: /model-cache
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        ports:
        - containerPort: 8000
          name: vllm
        - containerPort: 5557
          name: nixl
        readinessProbe:
          httpGet:
            path: /v1/models
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 5
        startupProbe:
          httpGet:
            path: /v1/models
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 60  # Allow up to 15 minutes for model loading
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: "32Gi"
            cpu: "8"
          requests:
            nvidia.com/gpu: "1"
            memory: "24Gi"
            cpu: "4"
        volumeMounts:
        - mountPath: /tmp
          name: tmp-cache
        - mountPath: /model-cache
          name: model-storage
        - mountPath: /dev/shm
          name: dshm
      volumes:
      - name: tmp-cache
        emptyDir:
          sizeLimit: 100Gi
      - name: model-storage
        emptyDir:
          sizeLimit: 200Gi  # Large cache for model and KV cache
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 8Gi  # Shared memory for efficient KV cache