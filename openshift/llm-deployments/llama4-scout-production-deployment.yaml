apiVersion: v1
kind: Secret
metadata:
  name: llm-d-hf-token
  namespace: llm-d
type: Opaque
stringData:
  HF_TOKEN: "hf_your_token_here"  # Replace with actual token
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama4-scout-production
  namespace: llm-d
  labels:
    app.kubernetes.io/name: llama4-scout-production
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: llama4-scout-modelservice
    llm-d.ai/role: decode
spec:
  replicas: 2  # Scale across available nodes
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: llama4-scout-modelservice
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: llama4-scout-modelservice
        llm-d.ai/role: decode
    spec:
      serviceAccount: llama4-scout-service-account
      serviceAccountName: llama4-scout-service-account
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
        args:
        - --model=microsoft/DialoGPT-large  # Larger model but CPU-friendly
        - --host=0.0.0.0
        - --port=8000
        - --enable-prefix-caching
        - --block-size=16
        - --max-model-len=2048
        - --max-num-seqs=256
        - --disable-log-requests
        - --trust-remote-code
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: llm-d-hf-token
              key: HF_TOKEN
        - name: PYTHONHASHSEED
          value: "42"
        - name: VLLM_CPU_KVCACHE_SPACE
          value: "8"  # 8GB KV cache on CPU
        - name: OMP_NUM_THREADS
          value: "8"
        - name: VLLM_WORKER_MULTIPROC_METHOD
          value: "spawn"
        ports:
        - containerPort: 8000
          name: vllm
        readinessProbe:
          httpGet:
            path: /v1/models
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 5
        startupProbe:
          httpGet:
            path: /v1/models
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 40  # Allow time for model loading
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        resources:
          limits:
            memory: "16Gi"  # Use available memory on m6a.xlarge
            cpu: "8"        # Use most of the CPU cores
          requests:
            memory: "12Gi"
            cpu: "4"
        volumeMounts:
        - mountPath: /tmp
          name: tmp-cache
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /model-cache
          name: model-storage
      volumes:
      - name: tmp-cache
        emptyDir:
          sizeLimit: 20Gi
      - name: model-storage
        emptyDir:
          sizeLimit: 50Gi  # Model and KV cache storage
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 4Gi  # Shared memory for efficiency
---
apiVersion: v1
kind: Service
metadata:
  name: llama4-scout-production-service
  namespace: llm-d
  labels:
    app.kubernetes.io/name: llama4-scout-production
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: llama4-scout-modelservice
    llm-d.ai/role: decode
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: vllm
  selector:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: llama4-scout-modelservice
    llm-d.ai/role: decode
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: llama4-scout-service-account
  namespace: llm-d
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: llama4-scout-model-access
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: llama4-scout-model-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: llama4-scout-model-access
subjects:
- kind: ServiceAccount
  name: llama4-scout-service-account
  namespace: llm-d