#!/usr/bin/env python3
"""
Test RL configuration files and validate LLM on/off settings.
Simple validation test that checks configs without complex imports.
"""

import os
import sys
import time
import yaml
import json
from pathlib import Path

def test_config_files():
    """Test that configuration files are valid and have correct settings."""
    print("\nðŸ“‹ TESTING CONFIGURATION FILES")
    print("=" * 50)
    
    configs_to_test = [
        ("configs/controller_ppo.yaml", True, "LLM-guided mode"),
        ("configs/controller_ppo_pure_rl.yaml", False, "Pure RL mode")
    ]
    
    results = {}
    
    for config_path, expected_llm_enabled, description in configs_to_test:
        print(f"\nðŸ” Testing {description}: {config_path}")
        
        if not os.path.exists(config_path):
            print(f"âŒ Config file not found: {config_path}")
            results[config_path] = False
            continue
        
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # Check key configuration sections
            required_sections = ['ppo', 'planner_integration', 'environment', 'training']
            missing_sections = [section for section in required_sections if section not in config]
            
            if missing_sections:
                print(f"âŒ Missing sections: {missing_sections}")
                results[config_path] = False
                continue
            
            # Check planner integration settings
            planner_config = config.get('planner_integration', {})
            use_planner = planner_config.get('use_planner', True)
            
            print(f"   ðŸ“Š Use planner: {use_planner}")
            print(f"   ðŸ“Š Enable visual: {planner_config.get('enable_visual', 'not set')}")
            print(f"   ðŸ“Š Use structured entities: {planner_config.get('use_structured_entities', 'not set')}")
            print(f"   ðŸ“Š Planner frequency: {planner_config.get('planner_frequency', 'not set')}")
            
            # Validate against expectations
            if use_planner != expected_llm_enabled:
                print(f"âŒ Expected use_planner={expected_llm_enabled}, got {use_planner}")
                results[config_path] = False
                continue
            
            # Check PPO settings
            ppo_config = config.get('ppo', {})
            print(f"   ðŸ“Š Learning rate: {ppo_config.get('learning_rate', 'not set')}")
            print(f"   ðŸ“Š Gamma: {ppo_config.get('gamma', 'not set')}")
            
            # Check environment settings
            env_config = config.get('environment', {})
            print(f"   ðŸ“Š Frame skip: {env_config.get('frame_skip', 'not set')}")
            print(f"   ðŸ“Š Max episode steps: {config.get('training', {}).get('max_episode_steps', 'not set')}")
            
            # Check logging settings
            logging_config = config.get('logging', {})
            project_name = logging_config.get('wandb_project', 'not set')
            print(f"   ðŸ“Š WandB project: {project_name}")
            
            print(f"âœ… {description} configuration is valid")
            results[config_path] = True
            
        except Exception as e:
            print(f"âŒ Failed to parse {config_path}: {e}")
            results[config_path] = False
    
    return results

def test_pyboy_basic():
    """Test basic PyBoy functionality to ensure RL environment can work."""
    print("\nðŸŽ® TESTING BASIC PYBOY FUNCTIONALITY")
    print("=" * 50)
    
    try:
        from pyboy import PyBoy
        
        rom_path = "roms/zelda_oracle_of_seasons.gbc"
        
        if not os.path.exists(rom_path):
            print(f"âŒ ROM file not found: {rom_path}")
            return False
        
        print("Creating PyBoy instance for RL testing...")
        pyboy = PyBoy(rom_path, window="null")
        
        print("âœ… PyBoy created successfully")
        
        # Test basic functionality needed for RL
        print("Testing RL-relevant functionality...")
        
        # Test memory access (needed for rewards)
        test_addresses = [0xC021, 0xC05B, 0xC6A5]  # Health, max health, rupees
        memory_values = []
        for addr in test_addresses:
            value = pyboy.memory[addr]
            memory_values.append(value)
        
        print(f"âœ… Memory access working: {memory_values}")
        
        # Test screen capture (may be needed for observations)
        screen = pyboy.screen.ndarray
        print(f"âœ… Screen capture working: {screen.shape}")
        
        # Test stepping (core RL functionality)
        for _ in range(100):
            pyboy.tick()
        
        print("âœ… Environment stepping working")
        
        # Test save state loading (for consistent starts)
        save_state_path = rom_path + ".state"
        if os.path.exists(save_state_path):
            with open(save_state_path, 'rb') as f:
                pyboy.load_state(f)
            print("âœ… Save state loading working")
        else:
            print("âš ï¸  Save state not found (optional)")
        
        pyboy.stop()
        
        print("âœ… All basic RL functionality tests passed")
        return True
        
    except Exception as e:
        print(f"âŒ PyBoy basic test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_action_space():
    """Test action space configuration for RL."""
    print("\nðŸŽ¯ TESTING ACTION SPACE CONFIGURATION")
    print("=" * 50)
    
    try:
        # Test that we can import and enumerate actions
        sys.path.insert(0, os.getcwd())
        
        # Define the actions directly to test
        actions = [
            "NOP", "UP", "DOWN", "LEFT", "RIGHT", 
            "A", "B", "START", "SELECT"
        ]
        
        print(f"âœ… Action space size: {len(actions)}")
        print(f"ðŸ“Š Available actions: {actions}")
        
        # Validate action space size is reasonable for RL
        assert len(actions) >= 8, "Need at least 8 actions for Zelda"
        assert len(actions) <= 16, "Too many actions may slow RL training"
        
        print("âœ… Action space validation passed")
        return True
        
    except Exception as e:
        print(f"âŒ Action space test failed: {e}")
        return False

def create_performance_profile():
    """Create a performance profile for both RL modes."""
    print("\nâš¡ CREATING PERFORMANCE PROFILE")
    print("=" * 50)
    
    profile = {
        'pure_rl_mode': {
            'description': 'RL without LLM guidance',
            'expected_performance': {
                'steps_per_second': '20-50 (estimated)',
                'memory_usage': 'Low (~100MB)',
                'cpu_usage': 'Medium (neural network + emulator)',
                'gpu_usage': 'Low-Medium (if available)'
            },
            'advantages': [
                'Faster execution',
                'No API dependencies',
                'Simpler debugging',
                'Lower resource usage'
            ],
            'disadvantages': [
                'Slower learning',
                'May get stuck without guidance',
                'Requires more exploration'
            ]
        },
        'llm_guided_mode': {
            'description': 'RL with LLM strategic guidance',
            'expected_performance': {
                'steps_per_second': '5-15 (estimated)',
                'memory_usage': 'Medium (~200-300MB)',
                'cpu_usage': 'High (neural network + emulator + state extraction)',
                'api_usage': 'Regular LLM API calls'
            },
            'advantages': [
                'Faster learning',
                'Strategic guidance',
                'Better exploration',
                'Domain knowledge integration'
            ],
            'disadvantages': [
                'Slower execution',
                'API dependencies',
                'More complex setup',
                'Higher resource usage'
            ]
        }
    }
    
    print("ðŸ“Š Performance Profile Summary:")
    for mode, details in profile.items():
        print(f"\n{mode.replace('_', ' ').title()}:")
        print(f"  Description: {details['description']}")
        print(f"  Expected SPS: {details['expected_performance']['steps_per_second']}")
        print(f"  Memory: {details['expected_performance']['memory_usage']}")
    
    return profile

def save_validation_results(config_results, pyboy_test, action_test, profile):
    """Save comprehensive validation results."""
    output_data = {
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
        'test_type': 'rl_configuration_validation',
        'configuration_tests': config_results,
        'pyboy_functionality': pyboy_test,
        'action_space_test': action_test,
        'performance_profile': profile,
        'summary': {
            'configs_passed': sum(config_results.values()),
            'total_configs': len(config_results),
            'basic_functionality': pyboy_test,
            'rl_ready': pyboy_test and action_test and all(config_results.values())
        }
    }
    
    output_file = "test_rl_config_validation.json"
    with open(output_file, 'w') as f:
        json.dump(output_data, f, indent=2)
    
    file_size = os.path.getsize(output_file)
    print(f"ðŸ’¾ Validation results saved: {output_file} ({file_size} bytes)")
    
    return output_file

def main():
    """Main validation function."""
    print("ðŸŽ¯ RL CONFIGURATION VALIDATION")
    print("=" * 60)
    print("Testing RL mode configurations and basic functionality")
    print()
    
    start_time = time.time()
    
    # Test configuration files
    config_results = test_config_files()
    
    # Test basic PyBoy functionality
    pyboy_test = test_pyboy_basic()
    
    # Test action space
    action_test = test_action_space()
    
    # Create performance profile
    profile = create_performance_profile()
    
    # Save results
    output_file = save_validation_results(config_results, pyboy_test, action_test, profile)
    
    total_time = time.time() - start_time
    
    print(f"\nðŸ VALIDATION SUMMARY")
    print("=" * 50)
    print(f"â±ï¸  Total validation time: {total_time:.2f} seconds")
    
    # Configuration results
    passed_configs = sum(config_results.values())
    total_configs = len(config_results)
    print(f"ðŸ“‹ Configuration files: {passed_configs}/{total_configs} passed")
    
    for config_path, passed in config_results.items():
        status = "âœ… VALID" if passed else "âŒ INVALID"
        config_name = os.path.basename(config_path)
        print(f"   {status} {config_name}")
    
    # Functionality results
    functionality_tests = {
        'PyBoy Basic': pyboy_test,
        'Action Space': action_test
    }
    
    passed_functionality = sum(functionality_tests.values())
    total_functionality = len(functionality_tests)
    print(f"ðŸ”§ Functionality tests: {passed_functionality}/{total_functionality} passed")
    
    for test_name, passed in functionality_tests.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"   {status} {test_name}")
    
    # Overall result
    all_passed = (passed_configs == total_configs and 
                  passed_functionality == total_functionality)
    
    if all_passed:
        print("\nðŸŽ‰ ALL VALIDATIONS PASSED!")
        print("âœ… Pure RL configuration: READY")
        print("âœ… LLM-guided configuration: READY")
        print("âœ… Basic RL functionality: WORKING")
        print("ðŸš€ System ready for RL training in both modes!")
        return True
    else:
        print("\nâŒ Some validations failed")
        print("Please check the errors above and fix configurations")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
