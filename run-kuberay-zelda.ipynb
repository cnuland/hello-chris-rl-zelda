{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ray RLlib Training for Zelda Oracle of Seasons\n",
        "\n",
        "This notebook deploys a Ray cluster on OpenShift/Kubernetes and submits a distributed training job.\n",
        "\n",
        "Based on the Double Dragon KubeRay implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è RBAC Setup Required\n",
        "\n",
        "**Before running this notebook**, you need to grant RBAC permissions to the service account.\n",
        "\n",
        "Run these commands in a terminal:\n",
        "\n",
        "```bash\n",
        "cd /Users/cnuland/hello-chris-rl-llm-zelda\n",
        "\n",
        "# Apply RBAC permissions\n",
        "oc apply -f ops/openshift/rbac.yaml\n",
        "\n",
        "# Verify permissions\n",
        "oc auth can-i list rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm\n",
        "oc auth can-i create rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm\n",
        "```\n",
        "\n",
        "All should return `yes` ‚úÖ\n",
        "\n",
        "**Then proceed with the cells below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install codeflare-sdk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated imports for newer codeflare_sdk versions\n",
        "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
        "from codeflare_sdk.cluster.auth import TokenAuthentication\n",
        "from ray.job_submission import JobSubmissionClient, JobStatus\n",
        "import os\n",
        "import time\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with OpenShift\n",
        "# Get your token: oc whoami -t\n",
        "# Get your server: oc cluster-info\n",
        "\n",
        "auth = TokenAuthentication(\n",
        "    token = 'YOUR_TOKEN_HERE',  # Replace with: oc whoami -t\n",
        "    server = 'YOUR_SERVER_HERE',  # Replace with: oc cluster-info\n",
        "    skip_tls=False\n",
        ")\n",
        "auth.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's check what parameters ClusterConfiguration actually accepts\n",
        "import inspect\n",
        "sig = inspect.signature(ClusterConfiguration.__init__)\n",
        "print(\"Available ClusterConfiguration parameters:\")\n",
        "for param_name, param in sig.parameters.items():\n",
        "    if param_name != 'self':\n",
        "        default = param.default if param.default != inspect.Parameter.empty else \"REQUIRED\"\n",
        "        print(f\"  {param_name}: {default}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Ray Cluster\n",
        "# ‚úÖ REUSING the Double Dragon image - it already has everything we need!\n",
        "# üéÆ PyBoy emulator works great on CPU - no GPUs needed!\n",
        "# ‚úÖ Ultra-minimal resources to guarantee scheduling on available nodes!\n",
        "\n",
        "cluster = Cluster(ClusterConfiguration(\n",
        "    name='zelda-rl',\n",
        "    namespace='zelda-hybrid-rl-llm',\n",
        "    num_workers=2,      # ‚úÖ Reduce from 3 to 2\n",
        "    \n",
        "    # Ultra-minimal CPU/memory (applies to both head and workers)\n",
        "    min_cpus=1,         # ‚úÖ Ultra-minimal: 1 CPU request\n",
        "    max_cpus=2,         # ‚úÖ Small limit: 2 CPUs max\n",
        "    min_memory=2,       # ‚úÖ Ultra-minimal: 2 GB request\n",
        "    max_memory=4,       # ‚úÖ Small limit: 4 GB max\n",
        "    \n",
        "    # No GPUs needed - PyBoy runs great on CPU!\n",
        "    num_gpus=0,\n",
        "    \n",
        "    # ‚úÖ Using your existing DD image\n",
        "    image=\"quay.io/cnuland/dd-kuberay-worker:latest\",\n",
        "))\n",
        "\n",
        "print(f\"‚úÖ Cluster configuration created:\")\n",
        "print(f\"   Name: {cluster.config.name}\")\n",
        "print(f\"   Namespace: {cluster.config.namespace}\")\n",
        "print(f\"   Workers: {cluster.config.num_workers}\")\n",
        "print(f\"   CPUs per pod: {cluster.config.min_cpus}-{cluster.config.max_cpus}\")\n",
        "print(f\"   Memory per pod: {cluster.config.min_memory}-{cluster.config.max_memory} GB\")\n",
        "print(f\"   Image: {cluster.config.image}\")\n",
        "print(f\"   Total pods: 3 (1 head + 2 workers)\")\n",
        "print(f\"   Total resources: 3-6 CPUs, 6-12 GB RAM\")\n",
        "print(f\"   Parallel games: 6 (3 per worker)\")\n",
        "print(f\"\\n‚úÖ Ultra-minimal resources - should fit on any available nodes!\")\n",
        "print(f\"‚úÖ Kueue queues (zelda-ray-queue ‚Üí ray-cluster-queue) are ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the Ray cluster (or connect to existing)\n",
        "from codeflare_sdk.cluster.cluster import CodeFlareClusterStatus\n",
        "\n",
        "print(\"üöÄ Creating/connecting to Ray cluster...\")\n",
        "cluster.up()\n",
        "\n",
        "# Check status immediately\n",
        "print(\"\\nüìä Checking cluster status...\")\n",
        "status_info = cluster.status()\n",
        "cluster_state = status_info[0] if isinstance(status_info, tuple) else status_info\n",
        "\n",
        "print(f\"Cluster state: {cluster_state}\")\n",
        "\n",
        "# Only wait if cluster is not already active\n",
        "if cluster_state == CodeFlareClusterStatus.READY:\n",
        "    print(\"‚úÖ Cluster is already READY!\")\n",
        "elif cluster_state in [CodeFlareClusterStatus.STARTING, CodeFlareClusterStatus.UNKNOWN]:\n",
        "    try:\n",
        "        print(\"\\n‚è≥ Waiting for cluster to be ready (max 10 minutes)...\")\n",
        "        cluster.wait_ready(timeout=600)\n",
        "        print(\"‚úÖ Cluster is ready!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error waiting for cluster: {e}\")\n",
        "        print(\"\\nüìã Cluster details:\")\n",
        "        print(cluster.details())\n",
        "else:\n",
        "    print(f\"‚úÖ Cluster status: {cluster_state}\")\n",
        "    \n",
        "# Show final cluster details\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL CLUSTER STATUS:\")\n",
        "print(\"=\"*60)\n",
        "cluster.details()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Check cluster status and image pull issues\n",
        "import subprocess\n",
        "\n",
        "print(\"üîç DIAGNOSTIC: Checking Ray cluster deployment...\\n\")\n",
        "\n",
        "try:\n",
        "    # Check RayCluster resource\n",
        "    print(\"1Ô∏è‚É£ RayCluster resource:\")\n",
        "    result = subprocess.run(\n",
        "        [\"oc\", \"get\", \"raycluster\", \"-n\", \"zelda-hybrid-rl-llm\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    print(result.stdout if result.returncode == 0 else result.stderr)\n",
        "    \n",
        "    # Check pods\n",
        "    print(\"\\n2Ô∏è‚É£ Pods in namespace:\")\n",
        "    result2 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"pods\", \"-n\", \"zelda-hybrid-rl-llm\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    print(result2.stdout if result2.returncode == 0 else result2.stderr)\n",
        "    \n",
        "    # Check for image pull errors in events\n",
        "    print(\"\\n3Ô∏è‚É£ Recent events (looking for ImagePullBackOff errors):\")\n",
        "    result3 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"events\", \"-n\", \"zelda-hybrid-rl-llm\", \n",
        "         \"--sort-by=.lastTimestamp\", \"--field-selector=type=Warning\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    events = result3.stdout if result3.returncode == 0 else result3.stderr\n",
        "    print(events if events.strip() else \"No warning events found\")\n",
        "    \n",
        "    # Check image pull secrets\n",
        "    print(\"\\n4Ô∏è‚É£ Image pull secrets in namespace:\")\n",
        "    result4 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"secrets\", \"-n\", \"zelda-hybrid-rl-llm\", \n",
        "         \"-o\", \"jsonpath={.items[?(@.type==\\\"kubernetes.io/dockerconfigjson\\\")].metadata.name}\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    secrets = result4.stdout if result4.returncode == 0 else result4.stderr\n",
        "    print(secrets if secrets.strip() else \"‚ö†Ô∏è  No dockerconfigjson secrets found!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error running diagnostic commands: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí° IMAGE PULL FIX:\")\n",
        "print(\"=\"*60)\n",
        "print(\"If you see 'ImagePullBackOff' errors, you need to:\")\n",
        "print(\"1. Check if quay.io/cnuland/dd-kuberay-worker:latest exists\")\n",
        "print(\"2. If private, create image pull secret:\")\n",
        "print(\"   oc create secret docker-registry quay-pull-secret \\\\\")\n",
        "print(\"     --docker-server=quay.io \\\\\")\n",
        "print(\"     --docker-username=YOUR_USERNAME \\\\\")\n",
        "print(\"     --docker-password=YOUR_PASSWORD \\\\\")\n",
        "print(\"     -n zelda-hybrid-rl-llm\")\n",
        "print(\"3. Then add to ClusterConfiguration:\")\n",
        "print(\"   image_pull_secrets=['quay-pull-secret']\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cluster details and submit job\n",
        "from ray.job_submission import JobSubmissionClient\n",
        "\n",
        "clusterDetails = cluster.details()\n",
        "print(f\"Ray Dashboard URL: {clusterDetails.dashboard}\")\n",
        "\n",
        "# Create Ray Job Submission Client\n",
        "# Extract the head service URL from cluster details\n",
        "ray_cluster_uri = f\"ray://zelda-rl-head-svc.zelda-hybrid-rl-llm.svc:10001\"\n",
        "ray_dashboard_url = clusterDetails.dashboard\n",
        "\n",
        "print(f\"Ray Cluster URI: {ray_cluster_uri}\")\n",
        "print(f\"Connecting to Ray dashboard: {ray_dashboard_url}\")\n",
        "\n",
        "# Create client using the dashboard URL\n",
        "client = JobSubmissionClient(ray_dashboard_url)\n",
        "\n",
        "# Configure environment variables\n",
        "env_vars = {\n",
        "    # S3/MinIO Storage (for model checkpoints and training results)\n",
        "    'S3_ACCESS_KEY_ID': 'YOUR_S3_KEY',              # Replace with your MinIO access key\n",
        "    'S3_SECRET_ACCESS_KEY': 'YOUR_S3_SECRET',       # Replace with your MinIO secret key\n",
        "    'S3_REGION_NAME': 'us-east-1',                  # MinIO region (usually us-east-1)\n",
        "    'S3_ENDPOINT_URL': 'YOUR_S3_ENDPOINT',          # Replace with MinIO endpoint URL\n",
        "    'S3_BUCKET_NAME': 'zelda-rl-checkpoints',       # Replace with your bucket name\n",
        "    \n",
        "    # LLM endpoint\n",
        "    'LLM_ENDPOINT': 'http://llm-d-infra-inference-gateway-istio.llm-d.svc.cluster.local/v1/chat/completions',\n",
        "    \n",
        "    # ROM path (relative to working_dir)\n",
        "    'ROM_PATH': 'roms/zelda_oracle_of_seasons.gbc',\n",
        "    \n",
        "    # Config paths\n",
        "    'ENV_CONFIG': 'configs/env.yaml',\n",
        "    'VISION_PROMPT_CONFIG': 'configs/vision_prompt.yaml',\n",
        "}\n",
        "\n",
        "# Submit training job\n",
        "# Ray's working_dir uploads our Zelda code to all workers!\n",
        "submission_id = client.submit_job(\n",
        "    entrypoint=\"python run-ray-zelda.py\",\n",
        "    runtime_env={\n",
        "        \"env_vars\": env_vars,\n",
        "        'working_dir': './',  # Uploads our Zelda code, configs, and ROMs\n",
        "        'pip': [],  # DD image already has everything!\n",
        "        \"excludes\": [\n",
        "            \"*.sh\", \"*.ipynb\", \"*.md\", \"__pycache__\", \"*.pyc\", \n",
        "            \"checkpoints/*\", \"training_runs/*\", \"strategic_test_results/*\",\n",
        "            \".git/*\", \"tmp/*\", \"HUD/*\", \"notebooks/*\", \"examples/*\"\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Job submitted successfully!\")\n",
        "print(f\"Submission ID: {submission_id}\")\n",
        "print(f\"Monitor at: {ray_dashboard_url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job status\n",
        "from ray.job_submission import JobStatus\n",
        "import time\n",
        "\n",
        "print(\"üîç Monitoring job status...\")\n",
        "print(\"(This will check every 30 seconds until completion)\")\n",
        "print()\n",
        "\n",
        "while True:\n",
        "    status = client.get_job_status(submission_id)\n",
        "    info = client.get_job_info(submission_id)\n",
        "    \n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] Job status: {status}\")\n",
        "    \n",
        "    if status in [JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.STOPPED]:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üìä Job completed with status: {status}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        # Get final logs\n",
        "        logs = client.get_job_logs(submission_id)\n",
        "        print(\"üìù Final job logs:\")\n",
        "        print(logs[-5000:] if len(logs) > 5000 else logs)  # Last 5000 chars\n",
        "        break\n",
        "    \n",
        "    time.sleep(30)  # Check every 30 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get job logs (anytime)\n",
        "logs = client.get_job_logs(submission_id)\n",
        "print(logs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üóëÔ∏è CLEANUP: Delete the Ray cluster (only run when completely done!)\n",
        "# \n",
        "# WARNING: This will delete the entire Ray cluster!\n",
        "# - All running training jobs will be stopped\n",
        "# - All pods will be terminated\n",
        "# - The RayCluster resource will be deleted\n",
        "#\n",
        "# Uncomment the line below to delete:\n",
        "# cluster.down()\n",
        "\n",
        "# To check if cluster was deleted:\n",
        "# !oc get raycluster -n zelda-hybrid-rl-llm\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
