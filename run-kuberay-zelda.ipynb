{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ray RLlib Training for Zelda Oracle of Seasons\n",
        "\n",
        "This notebook deploys a Ray cluster on OpenShift/Kubernetes and submits a distributed training job.\n",
        "\n",
        "Based on the Double Dragon KubeRay implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è RBAC Setup Required\n",
        "\n",
        "**Before running this notebook**, you need to grant RBAC permissions to the service account.\n",
        "\n",
        "Run these commands in a terminal:\n",
        "\n",
        "```bash\n",
        "cd /Users/cnuland/hello-chris-rl-llm-zelda\n",
        "\n",
        "# Apply RBAC permissions\n",
        "oc apply -f ops/openshift/rbac.yaml\n",
        "\n",
        "# Verify permissions\n",
        "oc auth can-i list rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm\n",
        "oc auth can-i create rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm\n",
        "```\n",
        "\n",
        "All should return `yes` ‚úÖ\n",
        "\n",
        "**Then proceed with the cells below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install codeflare-sdk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated imports for newer codeflare_sdk versions\n",
        "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
        "from codeflare_sdk.cluster.auth import TokenAuthentication\n",
        "from ray.job_submission import JobSubmissionClient, JobStatus\n",
        "import os\n",
        "import time\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with OpenShift\n",
        "# Get your token: oc whoami -t\n",
        "# Get your server: oc cluster-info\n",
        "\n",
        "auth = TokenAuthentication(\n",
        "    token = 'YOUR_TOKEN_HERE',  # Replace with: oc whoami -t\n",
        "    server = 'YOUR_SERVER_HERE',  # Replace with: oc cluster-info\n",
        "    skip_tls=False\n",
        ")\n",
        "auth.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's check what parameters ClusterConfiguration actually accepts\n",
        "import inspect\n",
        "sig = inspect.signature(ClusterConfiguration.__init__)\n",
        "print(\"Available ClusterConfiguration parameters:\")\n",
        "for param_name, param in sig.parameters.items():\n",
        "    if param_name != 'self':\n",
        "        default = param.default if param.default != inspect.Parameter.empty else \"REQUIRED\"\n",
        "        print(f\"  {param_name}: {default}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Ray Cluster\n",
        "# üöÄ MAXIMIZING ALL CLUSTER RESOURCES!\n",
        "# üéÆ Utilize all 6 GPU nodes + 2 CPU nodes for massive parallel training\n",
        "# ‚ö° High-performance configuration for production-scale training\n",
        "\n",
        "cluster = Cluster(ClusterConfiguration(\n",
        "    name='zelda-rl',\n",
        "    namespace='zelda-hybrid-rl-llm',\n",
        "    num_workers=7,      # üöÄ MAXIMIZE: Use all 6 GPU nodes + 1 large CPU node\n",
        "    \n",
        "    # üöÄ HIGH-PERFORMANCE: Maximize CPU/memory per node (g5.2xlarge capacity)\n",
        "    min_cpus=6,         # üöÄ Request 6 CPUs per pod (80% of g5.2xlarge)\n",
        "    max_cpus=7,         # üöÄ Allow up to 7 CPUs (90% of g5.2xlarge)\n",
        "    min_memory=24,      # üöÄ Request 24GB RAM per pod (80% of g5.2xlarge)\n",
        "    max_memory=28,      # üöÄ Allow up to 28GB RAM (90% of g5.2xlarge)\n",
        "    \n",
        "    # No direct GPU usage - preserve GPUs for LLM inference\n",
        "    num_gpus=0,\n",
        "    \n",
        "    # ‚úÖ Using your existing DD image\n",
        "    image=\"quay.io/cnuland/dd-kuberay-worker:latest\",\n",
        "))\n",
        "\n",
        "print(f\"üöÄ HIGH-PERFORMANCE CLUSTER CONFIGURATION:\")\n",
        "print(f\"   Name: {cluster.config.name}\")\n",
        "print(f\"   Namespace: {cluster.config.namespace}\")\n",
        "print(f\"   Workers: {cluster.config.num_workers}\")\n",
        "print(f\"   CPUs per pod: {cluster.config.min_cpus}-{cluster.config.max_cpus}\")\n",
        "print(f\"   Memory per pod: {cluster.config.min_memory}-{cluster.config.max_memory} GB\")\n",
        "print(f\"   Image: {cluster.config.image}\")\n",
        "print(f\"   Total pods: {1 + cluster.config.num_workers} (1 head + {cluster.config.num_workers} workers)\")\n",
        "print(f\"   Total resources: {(1 + cluster.config.num_workers) * cluster.config.min_cpus}-{(1 + cluster.config.num_workers) * cluster.config.max_cpus} CPUs, {(1 + cluster.config.num_workers) * cluster.config.min_memory}-{(1 + cluster.config.num_workers) * cluster.config.max_memory} GB RAM\")\n",
        "print(f\"   Parallel environments: {cluster.config.num_workers * 12} (12 per worker)\")\n",
        "print(f\"\\nüöÄ MAXIMUM RESOURCE UTILIZATION:\")\n",
        "print(f\"   - Utilizes all 6 GPU nodes + 1 large CPU node\")\n",
        "print(f\"   - ~{(1 + cluster.config.num_workers) * cluster.config.min_cpus} CPU cores total\")\n",
        "print(f\"   - ~{(1 + cluster.config.num_workers) * cluster.config.min_memory}GB RAM total\")\n",
        "print(f\"   - Preserves GPUs for LLM inference workloads\")\n",
        "print(f\"‚úÖ Kueue queues (zelda-ray-queue ‚Üí ray-cluster-queue) are ready!\")\n",
        "\n",
        "# Ensure Kueue LocalQueue exists for this namespace\n",
        "print(\"\\nüìù Setting up Kueue LocalQueue for Ray cluster scheduling...\")\n",
        "try:\n",
        "    !oc apply -f ops/openshift/zelda-localqueue.yaml\n",
        "    print(\"‚úÖ Kueue LocalQueue configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Could not configure Kueue LocalQueue: {e}\")\n",
        "    print(\"   This may cause Ray cluster creation to fail.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the Ray cluster (or connect to existing)\n",
        "from codeflare_sdk.cluster.cluster import CodeFlareClusterStatus\n",
        "\n",
        "print(\"üöÄ Creating/connecting to Ray cluster...\")\n",
        "print(\"\\nüìù Note: If cluster creation fails due to Kueue validation,\")\n",
        "print(\"   we'll add the required queue label after creation.\")\n",
        "\n",
        "try:\n",
        "    cluster.up()\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error creating cluster: {e}\")\n",
        "    if 'queue-name' in str(e):\n",
        "        print(\"\\nüîß Applying Kueue queue label to RayCluster...\")\n",
        "        # Apply the queue label to the RayCluster\n",
        "        !oc label raycluster {cluster.config.name} -n {cluster.config.namespace} kueue.x-k8s.io/queue-name=zelda-ray-queue --overwrite\n",
        "        print(\"‚úÖ Kueue queue label applied. Cluster should start now.\")\n",
        "    else:\n",
        "        raise e\n",
        "\n",
        "# Check status immediately\n",
        "print(\"\\nüìä Checking cluster status...\")\n",
        "try:\n",
        "    status_info = cluster.status()\n",
        "    cluster_state = status_info[0] if isinstance(status_info, tuple) else status_info\n",
        "except ValueError as e:\n",
        "    if 'suspended' in str(e):\n",
        "        print(\"‚ö†Ô∏è  Ray cluster is suspended by Kueue but may be operational\")\n",
        "        # Check if Ray cluster actually exists and has running pods\n",
        "        ray_status = !oc get rayclusters {cluster.config.name} -n {cluster.config.namespace} --no-headers\n",
        "        if ray_status and len(ray_status) > 0:\n",
        "            print(\"‚úÖ Ray cluster exists, checking pods...\")\n",
        "            ray_pods = !oc get pods -n {cluster.config.namespace} -l ray.io/cluster={cluster.config.name} --no-headers\n",
        "            if ray_pods and any('Running' in pod for pod in ray_pods):\n",
        "                print(\"‚úÖ Ray cluster has running pods - proceeding with job submission\")\n",
        "                cluster_state = \"READY_SUSPENDED\"  # Custom state\n",
        "            else:\n",
        "                print(\"‚ùå No running Ray pods found\")\n",
        "                cluster_state = \"UNKNOWN\"\n",
        "        else:\n",
        "            print(\"‚ùå Ray cluster not found\")\n",
        "            cluster_state = \"UNKNOWN\"\n",
        "    else:\n",
        "        raise e\n",
        "\n",
        "print(f\"Cluster state: {cluster_state}\")\n",
        "\n",
        "# Only wait if cluster is not already active\n",
        "if cluster_state == CodeFlareClusterStatus.READY:\n",
        "    print(\"‚úÖ Cluster is already READY!\")\n",
        "elif cluster_state in [CodeFlareClusterStatus.STARTING, CodeFlareClusterStatus.UNKNOWN]:\n",
        "    try:\n",
        "        print(\"\\n‚è≥ Waiting for cluster to be ready (max 10 minutes)...\")\n",
        "        cluster.wait_ready(timeout=600)\n",
        "        print(\"‚úÖ Cluster is ready!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error waiting for cluster: {e}\")\n",
        "        print(\"\\nüìã Cluster details:\")\n",
        "        print(cluster.details())\n",
        "else:\n",
        "    print(f\"‚úÖ Cluster status: {cluster_state}\")\n",
        "    \n",
        "# Show final cluster details\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL CLUSTER STATUS:\")\n",
        "print(\"=\"*60)\n",
        "cluster.details()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Check cluster status and image pull issues\n",
        "import subprocess\n",
        "\n",
        "print(\"üîç DIAGNOSTIC: Checking Ray cluster deployment...\\n\")\n",
        "\n",
        "try:\n",
        "    # Check RayCluster resource\n",
        "    print(\"1Ô∏è‚É£ RayCluster resource:\")\n",
        "    result = subprocess.run(\n",
        "        [\"oc\", \"get\", \"raycluster\", \"-n\", \"zelda-hybrid-rl-llm\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    print(result.stdout if result.returncode == 0 else result.stderr)\n",
        "    \n",
        "    # Check pods\n",
        "    print(\"\\n2Ô∏è‚É£ Pods in namespace:\")\n",
        "    result2 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"pods\", \"-n\", \"zelda-hybrid-rl-llm\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    print(result2.stdout if result2.returncode == 0 else result2.stderr)\n",
        "    \n",
        "    # Check for image pull errors in events\n",
        "    print(\"\\n3Ô∏è‚É£ Recent events (looking for ImagePullBackOff errors):\")\n",
        "    result3 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"events\", \"-n\", \"zelda-hybrid-rl-llm\", \n",
        "         \"--sort-by=.lastTimestamp\", \"--field-selector=type=Warning\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    events = result3.stdout if result3.returncode == 0 else result3.stderr\n",
        "    print(events if events.strip() else \"No warning events found\")\n",
        "    \n",
        "    # Check image pull secrets\n",
        "    print(\"\\n4Ô∏è‚É£ Image pull secrets in namespace:\")\n",
        "    result4 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"secrets\", \"-n\", \"zelda-hybrid-rl-llm\", \n",
        "         \"-o\", \"jsonpath={.items[?(@.type==\\\"kubernetes.io/dockerconfigjson\\\")].metadata.name}\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    secrets = result4.stdout if result4.returncode == 0 else result4.stderr\n",
        "    print(secrets if secrets.strip() else \"‚ö†Ô∏è  No dockerconfigjson secrets found!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error running diagnostic commands: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí° IMAGE PULL FIX:\")\n",
        "print(\"=\"*60)\n",
        "print(\"If you see 'ImagePullBackOff' errors, you need to:\")\n",
        "print(\"1. Check if quay.io/cnuland/dd-kuberay-worker:latest exists\")\n",
        "print(\"2. If private, create image pull secret:\")\n",
        "print(\"   oc create secret docker-registry quay-pull-secret \\\\\")\n",
        "print(\"     --docker-server=quay.io \\\\\")\n",
        "print(\"     --docker-username=YOUR_USERNAME \\\\\")\n",
        "print(\"     --docker-password=YOUR_PASSWORD \\\\\")\n",
        "print(\"     -n zelda-hybrid-rl-llm\")\n",
        "print(\"3. Then add to ClusterConfiguration:\")\n",
        "print(\"   image_pull_secrets=['quay-pull-secret']\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üñ•Ô∏è HUD DASHBOARD - ALREADY DEPLOYED\n",
        "# The HUD provides real-time visualization of training progress\n",
        "# - Real-time training metrics and progress tracking\n",
        "# - Shows current vision analysis from LLM\n",
        "# - Displays epoch, episode, rewards, and performance metrics\n",
        "\n",
        "print(\"üñ•Ô∏è Checking HUD Dashboard status...\")\n",
        "\n",
        "# Check if HUD is already running\n",
        "hud_status = !oc get pods -n zelda-hybrid-rl-llm -l app=zelda-hud --no-headers\n",
        "if hud_status and 'Running' in hud_status[0]:\n",
        "    print(\"‚úÖ HUD Dashboard is already running!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  HUD Dashboard not found. Deploying...\")\n",
        "    !oc apply -f ops/openshift/hud-deployment.yaml\n",
        "    !oc wait --for=condition=ready pod -l app=zelda-hud -n zelda-hybrid-rl-llm --timeout=120s\n",
        "\n",
        "# Get HUD route URL\n",
        "print(\"\\nüåê Getting HUD Dashboard URL...\")\n",
        "hud_route = !oc get route zelda-hud-route -n zelda-hybrid-rl-llm -o jsonpath='{.spec.host}'\n",
        "hud_url = f\"https://{hud_route[0]}\" if hud_route and hud_route[0] else \"Route not found\"\n",
        "\n",
        "print(f\"\\n‚úÖ HUD Dashboard ready!\")\n",
        "print(f\"üìä External Dashboard URL: {hud_url}\")\n",
        "print(f\"üîå Internal Service URL: http://zelda-hud-service.zelda-hybrid-rl-llm.svc.cluster.local:8086\")\n",
        "print(f\"\\nüí° Open the external dashboard URL in your browser to watch training!\")\n",
        "print(f\"   Ray training jobs will automatically connect to the internal service URL.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ RESET HUD SESSION\n",
        "# Clear any stale HUD sessions before starting new training jobs\n",
        "# This prevents \"409 Conflict\" errors when restarting jobs\n",
        "# Run this cell AFTER deploying HUD and BEFORE starting Ray jobs\n",
        "\n",
        "import requests\n",
        "\n",
        "print(\"üîÑ Resetting HUD session for new training job...\")\n",
        "hud_internal_url = \"http://zelda-hud-service.zelda-hybrid-rl-llm.svc.cluster.local:8086\"\n",
        "\n",
        "try:\n",
        "    response = requests.post(f\"{hud_internal_url}/api/reset_session\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        prev = data.get('previous_session')\n",
        "        if prev:\n",
        "            print(f\"‚úÖ HUD session cleared: {prev}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ HUD ready (no previous session)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Reset request returned: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not reset HUD (not critical): {e}\")\n",
        "\n",
        "print(\"üéØ HUD is ready for new training job!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cluster details and submit job\n",
        "from ray.job_submission import JobSubmissionClient\n",
        "\n",
        "clusterDetails = cluster.details()\n",
        "print(f\"üåê External Ray Dashboard URL: {clusterDetails.dashboard}\")\n",
        "print(\"   (Use this URL in your browser to view the dashboard)\")\n",
        "\n",
        "# Create Ray Job Submission Client\n",
        "# IMPORTANT: Use INTERNAL service URL (not external route) to avoid 403 Forbidden\n",
        "# The external HTTPS route requires authentication, but internal service doesn't\n",
        "ray_cluster_uri = \"ray://zelda-rl-head-svc.zelda-hybrid-rl-llm.svc:10001\"\n",
        "ray_dashboard_internal_url = \"http://zelda-rl-head-svc.zelda-hybrid-rl-llm.svc.cluster.local:8265\"\n",
        "\n",
        "print(f\"\\nüì° Ray Cluster URI: {ray_cluster_uri}\")\n",
        "print(f\"üîå Connecting to internal dashboard: {ray_dashboard_internal_url}\")\n",
        "\n",
        "# Create client using the INTERNAL dashboard URL\n",
        "client = JobSubmissionClient(ray_dashboard_internal_url)\n",
        "\n",
        "# Configure environment variables\n",
        "# üöÄ HIGH-PERFORMANCE SCALING CONFIGURATION\n",
        "env_vars = {\n",
        "    # S3/MinIO Storage (for model checkpoints and training results)\n",
        "    'S3_ACCESS_KEY_ID': 'admin',                    # MinIO access key\n",
        "    'S3_SECRET_ACCESS_KEY': 'zelda-rl-minio-2024',  # MinIO secret key\n",
        "    'S3_REGION_NAME': 'us-east-1',                  # MinIO region\n",
        "    'S3_ENDPOINT_URL': 'https://minio-api-route-minio-system.apps.rosa.rosa-58cx6.acrs.p3.openshiftapps.com',\n",
        "    'S3_BUCKET_NAME': 'zelda-rl-checkpoints',       # MinIO bucket for checkpoints\n",
        "    \n",
        "    # HUD Dashboard (for real-time visualization - using deployed service)\n",
        "    'HUD_URL': 'http://zelda-hud-service.zelda-hybrid-rl-llm.svc.cluster.local:8086',\n",
        "    \n",
        "    # LLM endpoint - Llama4Scout via AWS ELB (vision model needs /v1/chat/completions)\n",
        "    # NOTE: Host header routes to llama4scout - do NOT include /llama4scout/ in URL path!\n",
        "    'LLM_ENDPOINT': 'http://a48d81637dbfe42908405beb02605516-1010388078.us-east-2.elb.amazonaws.com/v1/chat/completions',\n",
        "    'LLM_HOST_HEADER': 'llama4scout.llm-d.local',  # Required Host header for routing\n",
        "    # Alternative: Disable LLM vision by setting empty endpoint\n",
        "    # 'LLM_ENDPOINT': '',  # Uncomment to disable LLM and run pure PPO\n",
        "    \n",
        "    # ROM path (relative to working_dir)\n",
        "    'ROM_PATH': 'roms/zelda_oracle_of_seasons.gbc',\n",
        "    \n",
        "    # Config paths\n",
        "    'ENV_CONFIG': 'configs/env.yaml',\n",
        "    'VISION_PROMPT_CONFIG': 'configs/vision_prompt.yaml',\n",
        "    \n",
        "    # üöÄ HIGH-PERFORMANCE CLUSTER SCALING (maximizes all resources)\n",
        "    'RAY_WORKERS': '84',              # üöÄ Scale to massive parallelism (7 Ray workers * 12 envs each)\n",
        "    'ENVS_PER_WORKER': '12',          # üöÄ 12 parallel environments per worker\n",
        "    'EPISODE_LENGTH': '61440',        # üöÄ Extended episodes (2048 * 30)\n",
        "    'BATCH_SIZE': '32768',            # üöÄ Large batch size for distributed training\n",
        "}\n",
        "\n",
        "# Submit training job\n",
        "# ROM files will be downloaded from S3 'roms' bucket at startup\n",
        "submission_id = client.submit_job(\n",
        "    entrypoint=\"bash start_ray_training.sh\",  # Downloads ROM from S3 then starts training\n",
        "    runtime_env={\n",
        "        \"env_vars\": env_vars,\n",
        "        'working_dir': './',  # Uploads our Zelda code, configs, and start_ray_training.sh\n",
        "        'pip': ['boto3', 'pyboy>=2.6.0', 'Pillow>=9.0.0', 'PyYAML>=6.0', 'requests>=2.31.0'],  # Added vision LLM dependencies\n",
        "        \"excludes\": [\n",
        "            \"*.ipynb\", \"*.md\", \"__pycache__\", \"*.pyc\", \n",
        "            \"checkpoints/*\", \"training_runs/*\", \"strategic_test_results/*\",\n",
        "            \".git/*\", \"tmp/*\", \"notebooks/*\", \"examples/*\",\n",
        "            \"HUD/templates/*\", \"HUD/static/*\", \"HUD/Containerfile\"\n",
        "            # NOTE: Not excluding *.sh - we need start_ray_training.sh!\n",
        "            # NOTE: Including HUD/hud_client.py for workers to connect to HUD server\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Job submitted successfully!\")\n",
        "print(f\"üìã Submission ID: {submission_id}\")\n",
        "print(f\"üåê Monitor at: {clusterDetails.dashboard}\")\n",
        "print(f\"\\nüí° TIP: Open the dashboard URL in your browser to watch training in real-time!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job status\n",
        "from ray.job_submission import JobStatus\n",
        "import time\n",
        "\n",
        "print(\"üîç Monitoring job status...\")\n",
        "print(\"(This will check every 30 seconds until completion)\")\n",
        "print()\n",
        "\n",
        "while True:\n",
        "    status = client.get_job_status(submission_id)\n",
        "    info = client.get_job_info(submission_id)\n",
        "    \n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] Job status: {status}\")\n",
        "    \n",
        "    if status in [JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.STOPPED]:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üìä Job completed with status: {status}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        # Get final logs\n",
        "        logs = client.get_job_logs(submission_id)\n",
        "        print(\"üìù Final job logs:\")\n",
        "        print(logs[-5000:] if len(logs) > 5000 else logs)  # Last 5000 chars\n",
        "        break\n",
        "    \n",
        "    time.sleep(30)  # Check every 30 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get job logs (anytime)\n",
        "logs = client.get_job_logs(submission_id)\n",
        "print(logs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üóëÔ∏è CLEANUP: Delete the Ray cluster (only run when completely done!)\n",
        "# \n",
        "# WARNING: This will delete the entire Ray cluster!\n",
        "# - All running training jobs will be stopped\n",
        "# - All pods will be terminated\n",
        "# - The RayCluster resource will be deleted\n",
        "#\n",
        "# Uncomment the line below to delete:\n",
        "# cluster.down()\n",
        "\n",
        "# To check if cluster was deleted:\n",
        "# !oc get raycluster -n zelda-hybrid-rl-llm\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
