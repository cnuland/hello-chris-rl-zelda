{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ray RLlib Training for Zelda Oracle of Seasons\n",
        "\n",
        "This notebook deploys a Ray cluster on OpenShift/Kubernetes and submits a distributed training job.\n",
        "\n",
        "Based on the Double Dragon KubeRay implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš ï¸ RBAC Setup Required\n",
        "\n",
        "**Before running this notebook**, you need to grant RBAC permissions to the service account.\n",
        "\n",
        "Run these commands in a terminal:\n",
        "\n",
        "```bash\n",
        "cd /Users/cnuland/hello-chris-rl-llm-zelda\n",
        "\n",
        "# Apply RBAC permissions\n",
        "oc apply -f ops/openshift/rbac.yaml\n",
        "\n",
        "# Verify permissions\n",
        "oc auth can-i list rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm\n",
        "oc auth can-i create rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm\n",
        "```\n",
        "\n",
        "All should return `yes` âœ…\n",
        "\n",
        "**Then proceed with the cells below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install codeflare-sdk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated imports for newer codeflare_sdk versions\n",
        "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
        "from codeflare_sdk.cluster.auth import TokenAuthentication\n",
        "from ray.job_submission import JobSubmissionClient, JobStatus\n",
        "import os\n",
        "import time\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with OpenShift\n",
        "# Get your token: oc whoami -t\n",
        "# Get your server: oc cluster-info\n",
        "\n",
        "auth = TokenAuthentication(\n",
        "    token = 'YOUR_TOKEN_HERE',  # Replace with: oc whoami -t\n",
        "    server = 'YOUR_SERVER_HERE',  # Replace with: oc cluster-info\n",
        "    skip_tls=False\n",
        ")\n",
        "auth.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's check what parameters ClusterConfiguration actually accepts\n",
        "import inspect\n",
        "sig = inspect.signature(ClusterConfiguration.__init__)\n",
        "print(\"Available ClusterConfiguration parameters:\")\n",
        "for param_name, param in sig.parameters.items():\n",
        "    if param_name != 'self':\n",
        "        default = param.default if param.default != inspect.Parameter.empty else \"REQUIRED\"\n",
        "        print(f\"  {param_name}: {default}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Ray Cluster\n",
        "# âœ… REUSING the Double Dragon image - it already has everything we need!\n",
        "# ðŸŽ® PyBoy emulator works great on CPU - no GPUs needed!\n",
        "# âœ… Ultra-minimal resources to guarantee scheduling on available nodes!\n",
        "\n",
        "cluster = Cluster(ClusterConfiguration(\n",
        "    name='zelda-rl',\n",
        "    namespace='zelda-hybrid-rl-llm',\n",
        "    num_workers=2,      # âœ… Reduce from 3 to 2\n",
        "    \n",
        "    # Ultra-minimal CPU/memory (applies to both head and workers)\n",
        "    min_cpus=1,         # âœ… Ultra-minimal: 1 CPU request\n",
        "    max_cpus=2,         # âœ… Small limit: 2 CPUs max\n",
        "    min_memory=2,       # âœ… Ultra-minimal: 2 GB request\n",
        "    max_memory=4,       # âœ… Small limit: 4 GB max\n",
        "    \n",
        "    # No GPUs needed - PyBoy runs great on CPU!\n",
        "    num_gpus=0,\n",
        "    \n",
        "    # âœ… Using your existing DD image\n",
        "    image=\"quay.io/cnuland/dd-kuberay-worker:latest\",\n",
        "))\n",
        "\n",
        "print(f\"âœ… Cluster configuration created:\")\n",
        "print(f\"   Name: {cluster.config.name}\")\n",
        "print(f\"   Namespace: {cluster.config.namespace}\")\n",
        "print(f\"   Workers: {cluster.config.num_workers}\")\n",
        "print(f\"   CPUs per pod: {cluster.config.min_cpus}-{cluster.config.max_cpus}\")\n",
        "print(f\"   Memory per pod: {cluster.config.min_memory}-{cluster.config.max_memory} GB\")\n",
        "print(f\"   Image: {cluster.config.image}\")\n",
        "print(f\"   Total pods: 3 (1 head + 2 workers)\")\n",
        "print(f\"   Total resources: 3-6 CPUs, 6-12 GB RAM\")\n",
        "print(f\"   Parallel games: 6 (3 per worker)\")\n",
        "print(f\"\\nâœ… Ultra-minimal resources - should fit on any available nodes!\")\n",
        "print(f\"âœ… Kueue queues (zelda-ray-queue â†’ ray-cluster-queue) are ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the Ray cluster\n",
        "print(\"Creating Ray cluster...\")\n",
        "cluster.up()\n",
        "\n",
        "# Check status immediately\n",
        "print(\"\\nChecking cluster status...\")\n",
        "status_info = cluster.status()\n",
        "print(f\"Status: {status_info}\")\n",
        "\n",
        "# Try to wait for ready (with better error handling)\n",
        "try:\n",
        "    print(\"\\nWaiting for cluster to be ready...\")\n",
        "    cluster.wait_ready(timeout=600)  # 10 minute timeout\n",
        "    print(\"âœ… Cluster is ready!\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Error waiting for cluster: {e}\")\n",
        "    print(\"\\nCluster details:\")\n",
        "    print(cluster.details())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Check cluster status and image pull issues\n",
        "import subprocess\n",
        "\n",
        "print(\"ðŸ” DIAGNOSTIC: Checking Ray cluster deployment...\\n\")\n",
        "\n",
        "try:\n",
        "    # Check RayCluster resource\n",
        "    print(\"1ï¸âƒ£ RayCluster resource:\")\n",
        "    result = subprocess.run(\n",
        "        [\"oc\", \"get\", \"raycluster\", \"-n\", \"zelda-hybrid-rl-llm\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    print(result.stdout if result.returncode == 0 else result.stderr)\n",
        "    \n",
        "    # Check pods\n",
        "    print(\"\\n2ï¸âƒ£ Pods in namespace:\")\n",
        "    result2 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"pods\", \"-n\", \"zelda-hybrid-rl-llm\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    print(result2.stdout if result2.returncode == 0 else result2.stderr)\n",
        "    \n",
        "    # Check for image pull errors in events\n",
        "    print(\"\\n3ï¸âƒ£ Recent events (looking for ImagePullBackOff errors):\")\n",
        "    result3 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"events\", \"-n\", \"zelda-hybrid-rl-llm\", \n",
        "         \"--sort-by=.lastTimestamp\", \"--field-selector=type=Warning\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    events = result3.stdout if result3.returncode == 0 else result3.stderr\n",
        "    print(events if events.strip() else \"No warning events found\")\n",
        "    \n",
        "    # Check image pull secrets\n",
        "    print(\"\\n4ï¸âƒ£ Image pull secrets in namespace:\")\n",
        "    result4 = subprocess.run(\n",
        "        [\"oc\", \"get\", \"secrets\", \"-n\", \"zelda-hybrid-rl-llm\", \n",
        "         \"-o\", \"jsonpath={.items[?(@.type==\\\"kubernetes.io/dockerconfigjson\\\")].metadata.name}\"],\n",
        "        capture_output=True, text=True, timeout=10\n",
        "    )\n",
        "    secrets = result4.stdout if result4.returncode == 0 else result4.stderr\n",
        "    print(secrets if secrets.strip() else \"âš ï¸  No dockerconfigjson secrets found!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error running diagnostic commands: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ’¡ IMAGE PULL FIX:\")\n",
        "print(\"=\"*60)\n",
        "print(\"If you see 'ImagePullBackOff' errors, you need to:\")\n",
        "print(\"1. Check if quay.io/cnuland/dd-kuberay-worker:latest exists\")\n",
        "print(\"2. If private, create image pull secret:\")\n",
        "print(\"   oc create secret docker-registry quay-pull-secret \\\\\")\n",
        "print(\"     --docker-server=quay.io \\\\\")\n",
        "print(\"     --docker-username=YOUR_USERNAME \\\\\")\n",
        "print(\"     --docker-password=YOUR_PASSWORD \\\\\")\n",
        "print(\"     -n zelda-hybrid-rl-llm\")\n",
        "print(\"3. Then add to ClusterConfiguration:\")\n",
        "print(\"   image_pull_secrets=['quay-pull-secret']\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cluster details and submit job\n",
        "from ray.job_submission import JobSubmissionClient\n",
        "\n",
        "clusterDetails = cluster.details()\n",
        "print(f\"Ray Dashboard URL: {clusterDetails.dashboard}\")\n",
        "\n",
        "# Create Ray Job Submission Client\n",
        "# Extract the head service URL from cluster details\n",
        "ray_cluster_uri = f\"ray://zelda-rl-head-svc.zelda-hybrid-rl-llm.svc:10001\"\n",
        "ray_dashboard_url = clusterDetails.dashboard\n",
        "\n",
        "print(f\"Ray Cluster URI: {ray_cluster_uri}\")\n",
        "print(f\"Connecting to Ray dashboard: {ray_dashboard_url}\")\n",
        "\n",
        "# Create client using the dashboard URL\n",
        "client = JobSubmissionClient(ray_dashboard_url)\n",
        "\n",
        "# Configure environment variables\n",
        "env_vars = {\n",
        "    # LLM endpoint (update with your actual LLM service endpoint)\n",
        "    'LLM_ENDPOINT': 'http://llm-d-infra-inference-gateway-istio.llm-d.svc.cluster.local/v1/chat/completions',\n",
        "    \n",
        "    # ROM path (relative to working_dir)\n",
        "    'ROM_PATH': 'roms/zelda_oracle_of_seasons.gbc',\n",
        "    \n",
        "    # Config paths\n",
        "    'ENV_CONFIG': 'configs/env.yaml',\n",
        "    'VISION_PROMPT_CONFIG': 'configs/vision_prompt.yaml',\n",
        "}\n",
        "\n",
        "# Submit training job\n",
        "# Ray's working_dir uploads our Zelda code to all workers!\n",
        "submission_id = client.submit_job(\n",
        "    entrypoint=\"python run-ray-zelda.py\",\n",
        "    runtime_env={\n",
        "        \"env_vars\": env_vars,\n",
        "        'working_dir': './',  # Uploads our Zelda code, configs, and ROMs\n",
        "        'pip': [],  # DD image already has everything!\n",
        "        \"excludes\": [\n",
        "            \"*.sh\", \"*.ipynb\", \"*.md\", \"__pycache__\", \"*.pyc\", \n",
        "            \"checkpoints/*\", \"training_runs/*\", \"strategic_test_results/*\",\n",
        "            \".git/*\", \"tmp/*\", \"HUD/*\", \"notebooks/*\", \"examples/*\"\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Job submitted successfully!\")\n",
        "print(f\"Submission ID: {submission_id}\")\n",
        "print(f\"Monitor at: {ray_dashboard_url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job status\n",
        "from ray.job_submission import JobStatus\n",
        "import time\n",
        "\n",
        "print(\"ðŸ” Monitoring job status...\")\n",
        "print(\"(This will check every 30 seconds until completion)\")\n",
        "print()\n",
        "\n",
        "while True:\n",
        "    status = client.get_job_status(submission_id)\n",
        "    info = client.get_job_info(submission_id)\n",
        "    \n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] Job status: {status}\")\n",
        "    \n",
        "    if status in [JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.STOPPED]:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ðŸ“Š Job completed with status: {status}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        # Get final logs\n",
        "        logs = client.get_job_logs(submission_id)\n",
        "        print(\"ðŸ“ Final job logs:\")\n",
        "        print(logs[-5000:] if len(logs) > 5000 else logs)  # Last 5000 chars\n",
        "        break\n",
        "    \n",
        "    time.sleep(30)  # Check every 30 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get job logs (anytime)\n",
        "logs = client.get_job_logs(submission_id)\n",
        "print(logs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up (when training is complete)\n",
        "# cluster.down()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
