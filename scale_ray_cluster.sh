#!/bin/bash
# Scale Ray cluster to maximize available resources

set -e

echo "‚ö° SCALING RAY CLUSTER FOR MAXIMUM PERFORMANCE"
echo "============================================="

# Get current cluster status
echo "üìä Current Ray cluster status:"
oc get rayclusters -n zelda-hybrid-rl-llm

# Scale up the cluster to use more resources per worker
echo ""
echo "üöÄ Scaling Ray cluster to use more CPU and memory per worker..."

# Update the RayCluster to use more resources
oc patch raycluster zelda-rl -n zelda-hybrid-rl-llm --type='merge' -p='
{
  "spec": {
    "workerGroupSpecs": [
      {
        "replicas": 3,
        "minReplicas": 1,
        "maxReplicas": 6,
        "groupName": "high-performance-workers",
        "rayStartParams": {},
        "template": {
          "spec": {
            "serviceAccountName": "zelda-rl-training",
            "containers": [
              {
                "name": "ray-worker",
                "image": "quay.io/cnuland/dd-kuberay-worker:latest",
                "imagePullPolicy": "Always",
                "resources": {
                  "limits": {
                    "cpu": "6",
                    "memory": "24Gi"
                  },
                  "requests": {
                    "cpu": "4",
                    "memory": "16Gi"  
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}'

echo "‚è≥ Waiting for Ray cluster to update..."
sleep 10

echo ""
echo "üìä Updated Ray cluster status:"
oc get rayclusters -n zelda-hybrid-rl-llm

echo ""
echo "üéØ New configuration should provide:"
echo "   - More CPU resources per worker"
echo "   - More memory per worker"
echo "   - Better training performance"

echo ""
echo "‚úÖ Ray cluster scaling complete!"
echo ""
echo "üí° Now you can run training with higher parallelization:"
echo "   export RAY_WORKERS=12"
echo "   export ENVS_PER_WORKER=12"
echo "   python run_direct_ray_training.py"