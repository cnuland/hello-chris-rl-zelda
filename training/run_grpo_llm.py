"""GRPO (Grouped Preference Optimization) training for LLM planner.

Trains the LLM planner using preference optimization based on rollout outcomes.
"""

import os
import json
import argparse
import asyncio
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import torch
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import AutoModelForCausalLMWithValueHead
import wandb

from emulator.zelda_env import ZeldaEnvironment
from agents.controller import HybridAgent, ControllerConfig
from agents.planner import ZeldaPlanner, PlannerConfig


@dataclass
class GRPOConfig:
    """Configuration for GRPO training."""
    batch_size: int = 8
    learning_rate: float = 1e-5
    num_epochs: int = 3
    rollout_length: int = 100
    preference_threshold: float = 0.1  # Minimum reward difference for preference
    temperature: float = 0.7
    max_length: int = 512


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="GRPO training for Zelda LLM planner")

    # Environment args
    parser.add_argument("--rom-path", type=str, required=True,
                       help="Path to Oracle of Seasons ROM file")
    parser.add_argument("--headless", action="store_true", default=True,
                       help="Run emulator headless")

    # Model args
    parser.add_argument("--model-name", type=str, default="microsoft/DialoGPT-medium",
                       help="Base model name (for local training)")
    parser.add_argument("--planner-endpoint", type=str,
                       default="http://zelda-planner-70b.zelda-ai.svc.cluster.local/v1/completions",
                       help="LLM planner endpoint URL")

    # Training args
    parser.add_argument("--total-episodes", type=int, default=1000,
                       help="Total training episodes")
    parser.add_argument("--rollouts-per-batch", type=int, default=4,
                       help="Number of rollouts per preference batch")
    parser.add_argument("--learning-rate", type=float, default=1e-5,
                       help="Learning rate")
    parser.add_argument("--batch-size", type=int, default=8,
                       help="Batch size")
    parser.add_argument("--num-epochs", type=int, default=3,
                       help="Number of training epochs per batch")

    # Logging args
    parser.add_argument("--exp-name", type=str, default="zelda_grpo",
                       help="Experiment name")
    parser.add_argument("--log-dir", type=str, default="logs",
                       help="Logging directory")
    parser.add_argument("--save-dir", type=str, default="checkpoints",
                       help="Checkpoint save directory")

    # Misc args
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed")
    parser.add_argument("--use-mock-planner", action="store_true",
                       help="Use mock planner for testing")

    return parser.parse_args()


class PreferenceData:
    """Container for preference training data."""

    def __init__(self):
        """Initialize preference data storage."""
        self.states: List[Dict[str, Any]] = []
        self.plans: List[Dict[str, Any]] = []
        self.rewards: List[float] = []
        self.outcomes: List[str] = []  # 'success', 'failure', 'neutral'

    def add_rollout(self, state: Dict[str, Any], plan: Dict[str, Any],
                   reward: float, outcome: str) -> None:
        """Add rollout data.

        Args:
            state: Game state that prompted the plan
            plan: Plan generated by LLM
            reward: Cumulative reward from executing the plan
            outcome: Categorical outcome assessment
        """
        self.states.append(state)
        self.plans.append(plan)
        self.rewards.append(reward)
        self.outcomes.append(outcome)

    def create_preference_pairs(self, threshold: float = 0.1) -> List[Tuple[int, int]]:
        """Create preference pairs based on reward differences.

        Args:
            threshold: Minimum reward difference for creating a pair

        Returns:
            List of (better_idx, worse_idx) pairs
        """
        pairs = []
        for i in range(len(self.rewards)):
            for j in range(i + 1, len(self.rewards)):
                reward_diff = abs(self.rewards[i] - self.rewards[j])
                if reward_diff > threshold:
                    if self.rewards[i] > self.rewards[j]:
                        pairs.append((i, j))  # i is better than j
                    else:
                        pairs.append((j, i))  # j is better than i
        return pairs

    def clear(self) -> None:
        """Clear all stored data."""
        self.states.clear()
        self.plans.clear()
        self.rewards.clear()
        self.outcomes.clear()


class GRPOTrainer:
    """GRPO trainer for LLM planner."""

    def __init__(self, args):
        """Initialize GRPO trainer.

        Args:
            args: Command line arguments
        """
        self.args = args

        # Set up directories
        os.makedirs(args.log_dir, exist_ok=True)
        os.makedirs(args.save_dir, exist_ok=True)

        # Initialize environment
        self.env = ZeldaEnvironment(
            rom_path=args.rom_path,
            headless=args.headless
        )

        # Initialize hybrid agent
        controller_config = ControllerConfig(use_planner=True)
        self.agent = HybridAgent(
            self.env,
            controller_config,
            use_mock_planner=args.use_mock_planner
        )

        # Initialize preference data storage
        self.preference_data = PreferenceData()

        # GRPO configuration
        self.grpo_config = GRPOConfig(
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            num_epochs=args.num_epochs
        )

        # Initialize local model for GRPO (if training locally)
        self.local_model = None
        self.tokenizer = None
        if not args.use_mock_planner:
            self._init_local_model()

        # Initialize wandb if available
        try:
            wandb.init(
                project="zelda-grpo",
                name=args.exp_name,
                config=vars(args)
            )
            self.use_wandb = True
        except Exception:
            print("Warning: Could not initialize wandb")
            self.use_wandb = False

        # Training state
        self.episode_count = 0

    def _init_local_model(self) -> None:
        """Initialize local model for GRPO training."""
        try:
            # For demonstration, we'll use a smaller model
            # In practice, you'd fine-tune the actual 70B model
            self.tokenizer = AutoTokenizer.from_pretrained(self.args.model_name)
            self.local_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.args.model_name)

            # Add special tokens if needed
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print(f"Initialized local model: {self.args.model_name}")
        except Exception as e:
            print(f"Warning: Could not initialize local model: {e}")

    async def collect_rollout(self, max_steps: int = 100) -> Tuple[float, Dict[str, Any], Dict[str, Any]]:
        """Collect a single rollout with planner integration.

        Args:
            max_steps: Maximum steps per rollout

        Returns:
            Tuple of (total_reward, initial_state, final_plan)
        """
        obs, info = self.env.reset()
        initial_state = info.get('structured_state', {})

        total_reward = 0.0
        final_plan = None
        step_count = 0

        while step_count < max_steps:
            structured_state = self.env.get_structured_state()

            # Get action from hybrid agent (includes LLM planning)
            action = await self.agent.act(obs, structured_state)

            # Execute action
            obs, reward, terminated, truncated, info = self.env.step(action)
            total_reward += reward
            step_count += 1

            # Store the last plan generated
            if hasattr(self.agent.controller, 'planner') and self.agent.controller.planner:
                # This is a simplified way to get the last plan
                # In practice, you'd need to modify the planner to store recent plans
                pass

            if terminated or truncated:
                break

        # Create a mock final plan for demonstration
        final_plan = {
            "subgoal": "Rollout completed",
            "reasoning": f"Executed {step_count} steps with reward {total_reward:.2f}",
            "macros": []
        }

        return total_reward, initial_state, final_plan

    def assess_outcome(self, reward: float, steps: int) -> str:
        """Assess rollout outcome.

        Args:
            reward: Total reward
            steps: Number of steps

        Returns:
            Outcome category
        """
        if reward > 5.0:
            return 'success'
        elif reward < -2.0:
            return 'failure'
        else:
            return 'neutral'

    async def collect_preference_batch(self) -> None:
        """Collect a batch of rollouts for preference learning."""
        print(f"Collecting preference batch ({self.args.rollouts_per_batch} rollouts)...")

        for rollout_idx in range(self.args.rollouts_per_batch):
            reward, initial_state, final_plan = await self.collect_rollout()
            outcome = self.assess_outcome(reward, 100)  # Assume 100 steps for simplicity

            self.preference_data.add_rollout(initial_state, final_plan, reward, outcome)

            print(f"Rollout {rollout_idx + 1}: Reward={reward:.2f}, Outcome={outcome}")

    def train_preference_model(self) -> Dict[str, float]:
        """Train the preference model using collected data.

        Returns:
            Training metrics
        """
        if not self.local_model or not self.tokenizer:
            # Can't train without local model
            return {"preference_loss": 0.0, "accuracy": 0.0}

        # Create preference pairs
        pairs = self.preference_data.create_preference_pairs(
            threshold=self.grpo_config.preference_threshold
        )

        if len(pairs) == 0:
            print("No preference pairs created - insufficient reward differences")
            return {"preference_loss": 0.0, "accuracy": 0.0}

        print(f"Created {len(pairs)} preference pairs for training")

        # Prepare training data
        better_inputs = []
        worse_inputs = []

        for better_idx, worse_idx in pairs:
            better_state = self.preference_data.states[better_idx]
            worse_state = self.preference_data.states[worse_idx]

            # Create input prompts (simplified)
            better_prompt = self._create_training_prompt(better_state)
            worse_prompt = self._create_training_prompt(worse_state)

            better_inputs.append(better_prompt)
            worse_inputs.append(worse_prompt)

        # Tokenize inputs
        better_tokens = self.tokenizer(
            better_inputs,
            padding=True,
            truncation=True,
            max_length=self.grpo_config.max_length,
            return_tensors="pt"
        )
        worse_tokens = self.tokenizer(
            worse_inputs,
            padding=True,
            truncation=True,
            max_length=self.grpo_config.max_length,
            return_tensors="pt"
        )

        # Simple preference training (placeholder implementation)
        # In practice, you'd implement proper preference optimization here
        optimizer = torch.optim.Adam(self.local_model.parameters(), lr=self.grpo_config.learning_rate)

        total_loss = 0.0
        num_batches = 0

        for epoch in range(self.grpo_config.num_epochs):
            optimizer.zero_grad()

            # Forward pass (simplified)
            better_outputs = self.local_model(**better_tokens)
            worse_outputs = self.local_model(**worse_tokens)

            # Placeholder loss computation
            # Real implementation would compute preference loss
            loss = torch.nn.functional.mse_loss(
                better_outputs.logits.mean(),
                worse_outputs.logits.mean() + 0.1  # Prefer better outcomes
            )

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / max(num_batches, 1)

        return {
            "preference_loss": avg_loss,
            "accuracy": 0.8,  # Placeholder
            "num_pairs": len(pairs)
        }

    def _create_training_prompt(self, state: Dict[str, Any]) -> str:
        """Create training prompt from game state.

        Args:
            state: Game state

        Returns:
            Formatted prompt string
        """
        # Simplified prompt creation
        player = state.get('player', {})
        resources = state.get('resources', {})

        prompt = f"""Game State:
Player: Health {player.get('health', 0)}/{player.get('max_health', 0)}, Position ({player.get('x', 0)}, {player.get('y', 0)})
Resources: {resources.get('rupees', 0)} rupees, {resources.get('keys', 0)} keys

Provide strategic plan:"""

        return prompt

    def log_metrics(self, metrics: Dict[str, float], episode: int) -> None:
        """Log training metrics.

        Args:
            metrics: Metrics dictionary
            episode: Current episode
        """
        # Print metrics
        print(f"Episode {episode}: {metrics}")

        # Log to wandb
        if self.use_wandb:
            wandb.log(metrics, step=episode)

    def save_checkpoint(self, episode: int) -> None:
        """Save training checkpoint.

        Args:
            episode: Current episode
        """
        if self.local_model:
            checkpoint_path = f"{self.args.save_dir}/grpo_checkpoint_{episode}.pt"
            torch.save(self.local_model.state_dict(), checkpoint_path)
            print(f"Saved GRPO checkpoint to {checkpoint_path}")

    async def train(self) -> None:
        """Main training loop."""
        print(f"Starting GRPO training for {self.args.total_episodes} episodes")
        print(f"ROM path: {self.args.rom_path}")

        episode = 0
        while episode < self.args.total_episodes:
            print(f"\n=== Episode {episode + 1}/{self.args.total_episodes} ===")

            # Collect preference batch
            await self.collect_preference_batch()

            # Train preference model
            metrics = self.train_preference_model()

            # Add episode info
            metrics['episode'] = episode
            metrics['total_rollouts'] = len(self.preference_data.rewards)

            # Log metrics
            self.log_metrics(metrics, episode)

            # Save checkpoint periodically
            if episode % 50 == 0:
                self.save_checkpoint(episode)

            # Clear preference data for next batch
            self.preference_data.clear()

            episode += 1
            self.episode_count = episode

        # Final save
        self.save_checkpoint(episode)

        # Cleanup
        print("GRPO training completed")
        self.env.close()
        await self.agent.close()

        if self.use_wandb:
            wandb.finish()


async def main():
    """Main function."""
    args = parse_args()
    trainer = GRPOTrainer(args)
    await trainer.train()


if __name__ == "__main__":
    asyncio.run(main())