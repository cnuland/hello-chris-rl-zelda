#!/bin/bash
# Production-Scale Zelda RL Training Script
# Maximizes all cluster resources for high-performance distributed training
#
# CLUSTER RESOURCES:
# - 6 √ó g5.2xlarge GPU nodes (7.5 CPUs, 30GB RAM, 1 GPU each)
# - 2 √ó Large CPU nodes (63.5 CPUs, 127GB RAM each)  
# - Total: ~174 CPUs, ~370GB RAM, 6 GPUs
#
# CONFIGURATION:
# - 84 parallel Ray workers (7 Kubernetes workers √ó 12 environments each)
# - 32GB batch size for distributed training
# - Extended episodes for better convergence
# - Preserves GPUs for LLM inference workloads

set -e  # Exit on error

echo "üöÄ PRODUCTION-SCALE ZELDA RL TRAINING"
echo "====================================="
echo ""
echo "üîß CLUSTER RESOURCE MAXIMIZATION:"
echo "   ‚Ä¢ Ray Workers: 84 (7 K8s pods √ó 12 envs each)"
echo "   ‚Ä¢ CPU Utilization: ~48 cores (7 pods √ó 6-7 CPUs each)"
echo "   ‚Ä¢ Memory Utilization: ~192GB (7 pods √ó 24-28GB each)"
echo "   ‚Ä¢ Batch Size: 32,768 (optimized for distributed training)"
echo "   ‚Ä¢ Episode Length: 61,440 steps (extended for convergence)"
echo ""
echo "‚ö° PERFORMANCE TARGETS:"
echo "   ‚Ä¢ Expected throughput: ~25,000 steps/second"
echo "   ‚Ä¢ Parallel environments: 84 simultaneous games"
echo "   ‚Ä¢ Training efficiency: 70-80% cluster utilization"
echo "   ‚Ä¢ GPUs preserved for LLM inference workloads"
echo ""

# Step 1: Validate prerequisites
echo "üìã Step 1: Validating prerequisites..."

# Check if in correct directory
if [ ! -f "run-ray-zelda.py" ]; then
    echo "‚ùå Error: run-ray-zelda.py not found. Please run from project root."
    exit 1
fi

# Check namespace exists
if ! oc get namespace zelda-hybrid-rl-llm >/dev/null 2>&1; then
    echo "‚ùå Error: Namespace 'zelda-hybrid-rl-llm' not found."
    echo "   Please create the namespace first:"
    echo "   oc create namespace zelda-hybrid-rl-llm"
    exit 1
fi

# Check RBAC permissions
echo "üîê Checking RBAC permissions..."
if ! oc auth can-i create rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm >/dev/null 2>&1; then
    echo "‚ö†Ô∏è  RBAC permissions need to be applied:"
    echo "   oc apply -f ops/openshift/rbac.yaml"
    echo ""
    read -p "Apply RBAC permissions now? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        oc apply -f ops/openshift/rbac.yaml
        echo "‚úÖ RBAC permissions applied"
    else
        echo "‚ùå RBAC permissions required. Exiting."
        exit 1
    fi
fi

echo "‚úÖ Prerequisites validated"
echo ""

# Step 2: Deploy HUD Dashboard
echo "üñ•Ô∏è  Step 2: Deploying HUD Dashboard..."
oc apply -f ops/openshift/hud-deployment.yaml

# Wait for HUD to be ready
echo "‚è≥ Waiting for HUD pod to be ready..."
oc wait --for=condition=ready pod -l app=zelda-hud -n zelda-hybrid-rl-llm --timeout=120s

# Get HUD route URL
HUD_ROUTE=$(oc get route zelda-hud-route -n zelda-hybrid-rl-llm -o jsonpath='{.spec.host}' 2>/dev/null || echo "")
if [ -n "$HUD_ROUTE" ]; then
    HUD_URL="https://$HUD_ROUTE"
    echo "‚úÖ HUD Dashboard deployed: $HUD_URL"
else
    echo "‚ö†Ô∏è  HUD route not found, using internal service"
    HUD_URL="http://zelda-hud-service.zelda-hybrid-rl-llm.svc.cluster.local:8086"
fi
echo ""

# Step 3: Configure production environment variables
echo "‚öôÔ∏è  Step 3: Configuring production scaling parameters..."

# Export high-performance configuration
export RAY_WORKERS=84              # 7 K8s workers √ó 12 environments each
export ENVS_PER_WORKER=12          # 12 parallel environments per worker
export EPISODE_LENGTH=61440        # Extended episodes (2048 * 30)
export BATCH_SIZE=32768            # Large batch size for distributed training
export HUD_URL="$HUD_URL"          # HUD dashboard URL

echo "‚úÖ Production configuration:"
echo "   RAY_WORKERS=$RAY_WORKERS"
echo "   ENVS_PER_WORKER=$ENVS_PER_WORKER"
echo "   EPISODE_LENGTH=$EPISODE_LENGTH"
echo "   BATCH_SIZE=$BATCH_SIZE"
echo "   Total parallel environments: $((RAY_WORKERS * ENVS_PER_WORKER))"
echo ""

# Step 4: Start production training
echo "üéÆ Step 4: Launching production-scale training..."
echo "üåê HUD Dashboard: $HUD_URL"
echo "üìä Monitor Ray dashboard at cluster details (see output below)"
echo ""

# Download ROM from S3 (if needed)
echo "üì• Downloading ROM files from S3..."
python init_rom.py

if [ $? -ne 0 ]; then
    echo "‚ùå Failed to download ROM files!"
    exit 1
fi

# Start production training
echo ""
echo "üöÄ STARTING PRODUCTION TRAINING..."
echo "‚ö° Expected performance: ~25,000 steps/second across $((RAY_WORKERS * ENVS_PER_WORKER)) parallel environments"
echo "üìà This configuration utilizes ~48 CPU cores and ~192GB RAM across the cluster"
echo ""

python run-ray-zelda.py

echo ""
echo "‚úÖ PRODUCTION TRAINING COMPLETED!"
echo "üìä Results saved to: ~/ray_results/zelda/"
echo "üñ•Ô∏è  HUD Dashboard (if still running): $HUD_URL"