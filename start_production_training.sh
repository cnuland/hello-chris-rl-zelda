#!/bin/bash
# Production-Scale Zelda RL Training Script
# Maximizes all cluster resources for high-performance distributed training
#
# CLUSTER RESOURCES:
# - 6 Ã— g5.2xlarge GPU nodes (7.5 CPUs, 30GB RAM, 1 GPU each)
# - 2 Ã— Large CPU nodes (63.5 CPUs, 127GB RAM each)  
# - Total: ~174 CPUs, ~370GB RAM, 6 GPUs
#
# CONFIGURATION:
# - 84 parallel Ray workers (7 Kubernetes workers Ã— 12 environments each)
# - 32GB batch size for distributed training
# - Extended episodes for better convergence
# - Preserves GPUs for LLM inference workloads

set -e  # Exit on error

echo "ğŸš€ PRODUCTION-SCALE ZELDA RL TRAINING"
echo "====================================="
echo ""
echo "ğŸ”§ CLUSTER RESOURCE MAXIMIZATION:"
echo "   â€¢ Ray Workers: 84 (7 K8s pods Ã— 12 envs each)"
echo "   â€¢ CPU Utilization: ~48 cores (7 pods Ã— 6-7 CPUs each)"
echo "   â€¢ Memory Utilization: ~192GB (7 pods Ã— 24-28GB each)"
echo "   â€¢ Batch Size: 32,768 (optimized for distributed training)"
echo "   â€¢ Episode Length: 61,440 steps (extended for convergence)"
echo ""
echo "âš¡ PERFORMANCE TARGETS:"
echo "   â€¢ Expected throughput: ~25,000 steps/second"
echo "   â€¢ Parallel environments: 84 simultaneous games"
echo "   â€¢ Training efficiency: 70-80% cluster utilization"
echo "   â€¢ GPUs preserved for LLM inference workloads"
echo ""

# Step 1: Validate prerequisites
echo "ğŸ“‹ Step 1: Validating prerequisites..."

# Check if in correct directory
if [ ! -f "run-ray-zelda.py" ]; then
    echo "âŒ Error: run-ray-zelda.py not found. Please run from project root."
    exit 1
fi

# Check namespace exists
if ! oc get namespace zelda-hybrid-rl-llm >/dev/null 2>&1; then
    echo "âŒ Error: Namespace 'zelda-hybrid-rl-llm' not found."
    echo "   Please create the namespace first:"
    echo "   oc create namespace zelda-hybrid-rl-llm"
    exit 1
fi

# Check RBAC permissions
echo "ğŸ” Checking RBAC permissions..."
if ! oc auth can-i create rayclusters --as=system:serviceaccount:zelda-hybrid-rl-llm:zelda-rl-training -n zelda-hybrid-rl-llm >/dev/null 2>&1; then
    echo "âš ï¸  RBAC permissions need to be applied:"
    echo "   oc apply -f ops/openshift/rbac.yaml"
    echo ""
    read -p "Apply RBAC permissions now? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        oc apply -f ops/openshift/rbac.yaml
        echo "âœ… RBAC permissions applied"
    else
        echo "âŒ RBAC permissions required. Exiting."
        exit 1
    fi
fi

echo "âœ… Prerequisites validated"
echo ""

# Step 2: Check HUD Dashboard Status
echo "ğŸ–¥ï¸  Step 2: Checking HUD Dashboard status..."

# Check if HUD is already running
if oc get pods -n zelda-hybrid-rl-llm -l app=zelda-hud --no-headers 2>/dev/null | grep -q "Running"; then
    echo "âœ… HUD Dashboard is already running!"
else
    echo "âš ï¸  HUD Dashboard not found. Deploying..."
    oc apply -f ops/openshift/hud-deployment.yaml
    echo "â³ Waiting for HUD pod to be ready..."
    oc wait --for=condition=ready pod -l app=zelda-hud -n zelda-hybrid-rl-llm --timeout=120s
fi

# Get HUD route URL
HUD_ROUTE=$(oc get route zelda-hud-route -n zelda-hybrid-rl-llm -o jsonpath='{.spec.host}' 2>/dev/null || echo "")
if [ -n "$HUD_ROUTE" ]; then
    HUD_URL_EXTERNAL="https://$HUD_ROUTE"
    echo "âœ… HUD Dashboard ready!"
    echo "ğŸ“Š External Dashboard: $HUD_URL_EXTERNAL"
else
    echo "âš ï¸  HUD external route not found"
    HUD_URL_EXTERNAL="Route not available"
fi

# Always use internal service URL for training jobs
HUD_URL="http://zelda-hud-service.zelda-hybrid-rl-llm.svc.cluster.local:8086"
echo "ğŸ”Œ Internal Service: $HUD_URL"
echo ""

# Step 3: Configure production environment variables
echo "âš™ï¸  Step 3: Configuring production scaling parameters..."

# Export high-performance configuration
export RAY_WORKERS=84              # 7 K8s workers Ã— 12 environments each
export ENVS_PER_WORKER=12          # 12 parallel environments per worker
export EPISODE_LENGTH=61440        # Extended episodes (2048 * 30)
export BATCH_SIZE=32768            # Large batch size for distributed training
export HUD_URL="$HUD_URL"          # HUD dashboard URL

echo "âœ… Production configuration:"
echo "   RAY_WORKERS=$RAY_WORKERS"
echo "   ENVS_PER_WORKER=$ENVS_PER_WORKER"
echo "   EPISODE_LENGTH=$EPISODE_LENGTH"
echo "   BATCH_SIZE=$BATCH_SIZE"
echo "   Total parallel environments: $((RAY_WORKERS * ENVS_PER_WORKER))"
echo ""

# Step 4: Start production training
echo "ğŸ® Step 4: Launching production-scale training..."
echo "ğŸŒ External HUD Dashboard: $HUD_URL_EXTERNAL"
echo "ğŸ”Œ Internal HUD Service: $HUD_URL"
echo "ğŸ“Š Monitor Ray dashboard at cluster details (see output below)"
echo ""

# Download ROM from S3 (if needed)
echo "ğŸ“¥ Downloading ROM files from S3..."
python init_rom.py

if [ $? -ne 0 ]; then
    echo "âŒ Failed to download ROM files!"
    exit 1
fi

# Start production training
echo ""
echo "ğŸš€ STARTING PRODUCTION TRAINING..."
echo "âš¡ Expected performance: ~25,000 steps/second across $((RAY_WORKERS * ENVS_PER_WORKER)) parallel environments"
echo "ğŸ“ˆ This configuration utilizes ~48 CPU cores and ~192GB RAM across the cluster"
echo ""

python run-ray-zelda.py

echo ""
echo "âœ… PRODUCTION TRAINING COMPLETED!"
echo "ğŸ“Š Results saved to: ~/ray_results/zelda/"
echo "ğŸ–¥ï¸  External HUD Dashboard: $HUD_URL_EXTERNAL"
echo "ğŸ”Œ Internal HUD Service: $HUD_URL"
