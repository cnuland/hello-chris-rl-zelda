apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama4scout-17b
  namespace: llm-d
  labels:
    app: llama4scout-17b
  annotations:
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/proxyCPU: "100m"
    sidecar.istio.io/proxyMemory: "128Mi"
spec:
  predictor:
    serviceAccountName: llama4scout-pd-sa
    minReplicas: 1
    maxReplicas: 1
    containers:
    - name: kserve-container
      image: ghcr.io/llm-d/llm-d:v0.2.0
      command:
      - python
      - -m
      - vllm.entrypoints.openai.api_server
      args:
      - --model=RedHatAI/Llama-4-Scout-17B-16E-Instruct
      - --host=0.0.0.0
      - --port=8080
      - --tensor-parallel-size=4
      - --enable-prefix-caching
      - --block-size=16
      - --gpu-memory-utilization=0.9
      - --max-model-len=8192
      - --disable-log-requests
      - --kv-cache-dtype=auto
      - --max-num-seqs=64
      - --trust-remote-code
      - --served-model-name=llama4scout-17b
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: llm-d-hf-token
            key: HF_TOKEN
      - name: CUDA_VISIBLE_DEVICES
        value: "0,1,2,3"
      - name: VLLM_LOGGING_LEVEL
        value: "INFO"
      - name: HF_HOME
        value: "/tmp/huggingface"
      - name: TRANSFORMERS_CACHE
        value: "/tmp/huggingface/transformers"
      - name: NCCL_DEBUG
        value: "WARN"
      ports:
      - containerPort: 8080
        name: h2c
        protocol: TCP
      resources:
        limits:
          cpu: "28"              # Use most of g5.2xlarge CPU
          memory: "120Gi"        # Use most of g5.2xlarge memory
          nvidia.com/gpu: "4"    # Use 4 GPUs for tensor parallelism
        requests:
          cpu: "24"
          memory: "96Gi"
          nvidia.com/gpu: "4"
      volumeMounts:
      - mountPath: /tmp/huggingface
        name: cache-volume
      - mountPath: /dev/shm
        name: shm
      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8080
        initialDelaySeconds: 180
        periodSeconds: 30
        timeoutSeconds: 10
        failureThreshold: 10
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 300
        periodSeconds: 60
        timeoutSeconds: 10
        failureThreshold: 3
    nodeSelector:
      node.kubernetes.io/instance-type: g5.2xlarge
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    volumes:
    - name: cache-volume
      emptyDir:
        sizeLimit: 200Gi  # Large cache for model storage
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi   # Large shared memory for GPU operations
---
# Update the gateway to use the InferenceService
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-gateway-config
  namespace: llm-d
data:
  nginx.conf: |
    events {
        worker_connections 1024;
    }
    
    upstream llm_backend {
        server llama4scout-17b-predictor-default.llm-d.svc.cluster.local:80;
    }
    
    server {
        listen 80;
        location / {
            proxy_pass http://llm_backend;
            proxy_set_header Host llama4scout-17b-predictor-default.llm-d.svc.cluster.local;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_connect_timeout 60s;
            proxy_send_timeout 120s;
            proxy_read_timeout 120s;
        }
        
        location /health {
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }