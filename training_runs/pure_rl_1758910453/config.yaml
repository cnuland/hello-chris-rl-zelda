environment:
  frame_skip: 4
  max_episode_steps: 10000
  normalize_observations: true
  observation_type: vector
exploration:
  curiosity_coeff: 0.1
  epsilon_schedule:
    decay_steps: 100000
    final: 0.02
    initial: 0.2
  use_curiosity: true
planner_integration:
  auto_load_save_state: true
  enable_visual: false
  use_planner: false
  use_structured_entities: false
ppo:
  batch_size: 32
  clip_epsilon: 0.2
  entropy_coeff: 0.01
  gae_lambda: 0.95
  gamma: 0.99
  learning_rate: 0.0003
  max_grad_norm: 0.5
  num_steps: 128
  update_epochs: 4
  value_loss_coeff: 0.5
rewards:
  death_penalty: -3.0
  health_gain_reward: 0.2
  health_loss_penalty: -0.1
  key_reward: 0.5
  movement_reward: 0.001
  rupee_reward: 0.01
  time_penalty: -0.0001
training:
  early_termination:
    health_zero: true
    stuck_threshold: 1000
  eval_frequency: 10000
  log_frequency: 100
  max_episode_steps: 10000
  save_frequency: 10000
  total_timesteps: 20
