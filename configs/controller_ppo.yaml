# PPO Controller Configuration for Zelda Oracle of Seasons

# Network architecture
network:
  hidden_size: 256
  num_layers: 3
  activation: "relu"
  use_orthogonal_init: true
  init_gain: 1.0

# PPO hyperparameters
ppo:
  learning_rate: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_loss_coeff: 0.5
  entropy_coeff: 0.01
  max_grad_norm: 0.5

  # Training schedule
  num_steps: 128  # Steps per rollout
  update_epochs: 4  # PPO update epochs per rollout
  batch_size: 32  # Minibatch size for updates

# Learning rate scheduling
lr_schedule:
  type: "linear"  # "constant", "linear", "cosine"
  decay_steps: 1000000
  min_lr: 1e-5

# Reward configuration
rewards:
  # Core gameplay rewards
  rupee_reward: 0.01
  key_reward: 0.5
  boss_defeat_reward: 2.0
  death_penalty: -3.0

  # Shaping rewards
  movement_reward: 0.001  # Small reward for movement (anti-idle)
  exploration_reward: 0.1  # Reward for visiting new areas
  time_penalty: -0.0001  # Small penalty per step (efficiency)

  # Health-based rewards
  health_loss_penalty: -0.1  # Per heart lost
  health_gain_reward: 0.2   # Per heart gained

  # Item collection rewards
  item_collection_reward: 0.3
  treasure_chest_reward: 0.5

  # Progress rewards
  dungeon_progress_reward: 1.0
  essence_collection_reward: 5.0

# Training configuration
training:
  total_timesteps: 1000000
  eval_frequency: 50000
  save_frequency: 100000
  log_frequency: 1000

  # Episode management
  max_episode_steps: 10000
  early_termination:
    health_zero: true
    stuck_threshold: 1000  # Terminate if no progress for N steps

# Environment configuration
environment:
  frame_skip: 4
  action_repeat: 1
  observation_type: "vector"  # "vector", "image", "hybrid"

  # State preprocessing
  normalize_observations: true
  clip_rewards: false
  reward_scaling: 1.0

# Planner integration
planner_integration:
  use_planner: true
  planner_frequency: 100  # Call planner every N steps
  macro_timeout: 200  # Max steps to execute a macro

  # Planner override conditions
  override_on_low_health: true
  override_health_threshold: 0.3
  override_on_stuck: true
  override_stuck_threshold: 50

# Exploration configuration
exploration:
  epsilon_schedule:
    initial: 0.1
    final: 0.01
    decay_steps: 500000

  # Intrinsic motivation
  use_curiosity: false
  curiosity_coeff: 0.1

  # Random action injection
  random_action_prob: 0.05

# Logging and monitoring
logging:
  wandb_project: "zelda-rl"
  tensorboard_dir: "logs/tensorboard"
  log_video: false
  video_frequency: 100000

  # Metrics to track
  track_metrics:
    - "episode_reward"
    - "episode_length"
    - "rupees_collected"
    - "keys_collected"
    - "bosses_defeated"
    - "exploration_rate"
    - "planner_calls"
    - "macro_success_rate"

# Checkpointing
checkpointing:
  save_dir: "checkpoints"
  keep_last_n: 5
  save_optimizer: true

# Evaluation
evaluation:
  num_episodes: 10
  deterministic: true
  render: false

# Hardware configuration
hardware:
  device: "auto"  # "cpu", "cuda", "auto"
  num_workers: 1  # For parallel environments
  pin_memory: true