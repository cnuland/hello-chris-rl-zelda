# Smart Arbitration Configuration - OPTIMIZED FOR MLX LOCAL LLM
# Qwen2.5-14B-Instruct-4bit with ~1.3 second response times

# PPO Base Configuration
ppo:
  learning_rate: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_loss_coeff: 0.5
  entropy_coeff: 0.01
  max_grad_norm: 0.5

# üçé MLX-OPTIMIZED SMART ARBITRATION 
planner_integration:
  use_planner: true
  
  # üß† BALANCED ARBITRATION - Optimized for 1.3s MLX responses
  use_smart_arbitration: true
  base_planner_frequency: 100          # Every ~7 seconds (was 150)
  min_planner_frequency: 60            # Every ~4 seconds minimum  
  max_planner_frequency: 200           # Every ~13 seconds maximum
  
  # üéØ CONTEXT TRIGGERS - Optimized for MLX performance
  trigger_on_new_room: true            # Exploration guidance
  trigger_on_low_health: true          # Emergency response
  trigger_on_stuck: true               # Progress assistance
  trigger_on_npc_interaction: true     # Dialogue optimization
  trigger_on_dungeon_entrance: true    # Strategic planning
  
  # ‚ö° MLX-OPTIMIZED THRESHOLDS
  low_health_threshold: 0.25           
  stuck_threshold: 60                  # Balanced stuck detection
  macro_timeout: 50                    # Faster than original but reasonable
  
  # üñ•Ô∏è MLX SERVER SETTINGS
  endpoint_url: "http://localhost:8000/v1/chat/completions"
  model_name: "mlx-community/Qwen2.5-14B-Instruct-4bit"
  max_tokens: 100                      # Focused responses
  temperature: 0.3                     # Consistent for training
  timeout: 10.0                        # Account for 1-2s response
  max_retries: 2                       # Local server reliability
  
  # üìà MLX PERFORMANCE TRACKING  
  track_arbitration_performance: true
  arbitration_success_window: 75       # Balanced feedback loop

# Environment optimized for MLX LLM calls
environment:
  frame_skip: 4
  action_repeat: 1
  observation_type: "vector"
  normalize_observations: true
  max_episode_steps: 8000

# Enhanced Rewards (compatible with MLX LLM guidance)
rewards:
  time_penalty: -0.0001
  movement_reward: 0.001
  death_penalty: -3.0
  
  # EXPLORATION REWARDS - Enhanced by smart MLX guidance
  room_discovery_reward: 10.0
  dungeon_discovery_reward: 25.0
  dungeon_bonus: 5.0
  npc_interaction_reward: 15.0
  
  # MLX-specific rewards
  efficient_guidance_bonus: 2.0        # Bonus for following good LLM advice
  quick_decision_bonus: 0.5            # Small bonus for sub-2s responses

# Training optimized for MLX arbitration
training:
  total_timesteps: 400000
  eval_frequency: 20000
  save_frequency: 40000
  log_frequency: 500                   # Regular logging
  
  max_episode_steps: 6000              # Balanced episodes

# üìä MLX LLM SPECIFIC METRICS
logging:
  track_metrics:
    - "episode_reward"
    - "episode_length"
    - "exploration_rate"
    - "mlx_calls_per_episode"          # Track MLX usage frequency
    - "arbitration_latency"            # Monitor 1.3s performance
    - "context_trigger_effectiveness"   # How often triggers help
    - "mlx_server_uptime"              # Server availability
    - "smart_decision_rate"            # Quality of LLM decisions
    - "emergency_response_success"     # Critical health guidance
    - "exploration_acceleration"       # MLX-guided discovery rate

# Hardware
hardware:
  device: "auto"
  num_workers: 1
