---
apiVersion: v1
kind: Service
metadata:
  name: llama4scout-17b-headless
  namespace: llm-d
  labels:
    app: llama4scout-17b-distributed
spec:
  clusterIP: None  # Headless service for pod-to-pod communication
  ports:
  - port: 8000
    targetPort: 8000
    name: api
  - port: 29500
    targetPort: 29500
    name: nccl
  selector:
    app: llama4scout-17b-distributed
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: llama4scout-17b-distributed
  namespace: llm-d
  labels:
    app: llama4scout-17b-distributed
spec:
  serviceName: llama4scout-17b-headless
  replicas: 8  # Use 8 GPUs for tensor parallelism across g5.2xlarge nodes
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: llama4scout-17b-distributed
  template:
    metadata:
      labels:
        app: llama4scout-17b-distributed
    spec:
      serviceAccountName: llama4scout-pd-sa
      nodeSelector:
        node.kubernetes.io/instance-type: g5.2xlarge
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: ["llama4scout-17b-distributed"]
            topologyKey: kubernetes.io/hostname
      initContainers:
      - name: wait-for-peers
        image: busybox:latest
        command:
        - sh
        - -c
        - |
          echo "Waiting for all pods to be scheduled and ready..."
          # Wait longer for higher-numbered pods to allow for sequential startup
          POD_INDEX=$(echo $HOSTNAME | grep -o '[0-9]*$')
          WAIT_TIME=$((60 + POD_INDEX * 10))
          echo "Pod $POD_INDEX waiting $WAIT_TIME seconds"
          sleep $WAIT_TIME
          echo "Initialization wait complete for pod $POD_INDEX"
      containers:
      - name: vllm
        image: ghcr.io/llm-d/llm-d:v0.2.0
        command:
        - sh
        - -c
        - |
          # Set up distributed environment
          export MASTER_ADDR="llama4scout-17b-distributed-0.llama4scout-17b-headless.llm-d.svc.cluster.local"
          export MASTER_PORT="29500"
          export WORLD_SIZE="8"
          
          # Extract rank from hostname (llama4scout-17b-distributed-N)
          export RANK=$(echo $HOSTNAME | grep -o '[0-9]*$')
          
          echo "Starting node $RANK of $WORLD_SIZE, master: $MASTER_ADDR:$MASTER_PORT"
          
          # Start vLLM with tensor parallelism
          python -m vllm.entrypoints.openai.api_server \
            --model=RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic \
            --host=0.0.0.0 \
            --port=8000 \
            --tensor-parallel-size=8 \
            --enable-prefix-caching \
            --block-size=16 \
            --gpu-memory-utilization=0.8 \
            --max-model-len=8192 \
            --disable-log-requests \
            --kv-cache-dtype=auto \
            --max-num-seqs=64 \
            --trust-remote-code \
            --served-model-name=llama4scout-17b
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: llm-d-hf-token
              key: HF_TOKEN
        - name: PYTHONHASHSEED
          value: "42"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: HF_HOME
          value: "/tmp/huggingface"
        - name: TRANSFORMERS_CACHE
          value: "/tmp/huggingface/transformers"
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        ports:
        - containerPort: 8000
          name: api
        - containerPort: 29500
          name: nccl
        resources:
          limits:
            cpu: "6"
            memory: "26Gi"
            nvidia.com/gpu: "1"  # Each pod uses 1 GPU
          requests:
            cpu: "4"
            memory: "20Gi"
            nvidia.com/gpu: "1"  # Each pod uses 1 GPU
        volumeMounts:
        - mountPath: /tmp/huggingface
          name: cache-volume
        - mountPath: /dev/shm
          name: shm
        readinessProbe:
          httpGet:
            path: /v1/models
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 15
          failureThreshold: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 360
          periodSeconds: 60
          timeoutSeconds: 15
          failureThreshold: 3
      volumes:
      - name: cache-volume
        emptyDir:
          sizeLimit: 80Gi
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 8Gi
---
apiVersion: v1
kind: Service
metadata:
  name: llama4scout-17b-service
  namespace: llm-d
  labels:
    app: llama4scout-17b-distributed
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    name: api
  selector:
    app: llama4scout-17b-distributed
    statefulset.kubernetes.io/pod-name: llama4scout-17b-distributed-0  # Route to rank 0
---
# Update the gateway to point to the new service
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-gateway-config
  namespace: llm-d
data:
  nginx.conf: |
    events {
        worker_connections 1024;
    }
    
    upstream llm_backend {
        server llama4scout-17b-service.llm-d.svc.cluster.local:8000;
    }
    
    server {
        listen 80;
        location / {
            proxy_pass http://llm_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_connect_timeout 60s;
            proxy_send_timeout 120s;
            proxy_read_timeout 120s;
        }
        
        location /health {
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }