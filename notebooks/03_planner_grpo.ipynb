{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zelda LLM Planner GRPO Training\n",
    "\n",
    "This notebook demonstrates Grouped Preference Optimization (GRPO) training for the LLM planner.\n",
    "It collects rollouts using different plans and optimizes the planner based on outcome preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path('../').resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Configuration\n",
    "CONFIG = {\n",
    "    'rom_path': '../roms/zelda_oracle_of_seasons.gbc',\n",
    "    'total_episodes': 50,        # Reduced for notebook demo\n",
    "    'rollouts_per_batch': 4,     # Number of rollouts per preference batch\n",
    "    'rollout_length': 100,       # Steps per rollout\n",
    "    'preference_threshold': 0.5, # Minimum reward difference for preference\n",
    "    'use_mock_planner': True,    # Use mock planner for demo\n",
    "    'save_data': True,           # Save preference data for analysis\n",
    "}\n",
    "\n",
    "# Check ROM path\n",
    "rom_path = Path(CONFIG['rom_path'])\n",
    "if not rom_path.exists():\n",
    "    rom_dir = project_root / 'roms'\n",
    "    rom_files = list(rom_dir.glob('*.gbc')) + list(rom_dir.glob('*.gb'))\n",
    "    if rom_files:\n",
    "        CONFIG['rom_path'] = str(rom_files[0])\n",
    "        print(f\"Using ROM: {CONFIG['rom_path']}\")\n",
    "    else:\n",
    "        print(\"‚ùå No ROM file found!\")\n",
    "        CONFIG['rom_path'] = None\n",
    "\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['rom_path']:\n",
    "    from emulator.zelda_env import ZeldaEnvironment\n",
    "    from agents.controller import HybridAgent, ControllerConfig\n",
    "    from agents.planner import MockPlanner\n",
    "    from training.run_grpo_llm import PreferenceData\n",
    "    \n",
    "    # Create environment\n",
    "    env = ZeldaEnvironment(CONFIG['rom_path'], headless=True)\n",
    "    \n",
    "    # Create agent with planner\n",
    "    agent_config = ControllerConfig(\n",
    "        use_planner=True,\n",
    "        planner_frequency=20  # Call planner more frequently for GRPO\n",
    "    )\n",
    "    agent = HybridAgent(env, agent_config, use_mock_planner=True)\n",
    "    \n",
    "    # Create separate planner for experiments\n",
    "    planner = MockPlanner()\n",
    "    \n",
    "    # Create preference data storage\n",
    "    preference_data = PreferenceData()\n",
    "    \n",
    "    print(\"‚úÖ Components initialized\")\n",
    "    print(f\"Environment: {env}\")\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(f\"Planner: {planner}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping initialization - no ROM file available\")\n",
    "    env = None\n",
    "    agent = None\n",
    "    planner = None\n",
    "    preference_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanVariator:\n",
    "    \"\"\"Generate variations of plans for preference learning.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.plan_templates = [\n",
    "            {\n",
    "                \"subgoal\": \"Explore systematically\",\n",
    "                \"reasoning\": \"Methodical exploration to find items and secrets\",\n",
    "                \"macros\": [{\"action_type\": \"EXPLORE_ROOM\", \"parameters\": {}, \"priority\": 1.0}]\n",
    "            },\n",
    "            {\n",
    "                \"subgoal\": \"Aggressive movement\",\n",
    "                \"reasoning\": \"Quick movement to cover more ground\",\n",
    "                \"macros\": [{\"action_type\": \"MOVE_TO\", \"parameters\": {\"x\": 10, \"y\": 0}, \"priority\": 1.0}]\n",
    "            },\n",
    "            {\n",
    "                \"subgoal\": \"Attack focus\",\n",
    "                \"reasoning\": \"Focus on combat and enemy engagement\",\n",
    "                \"macros\": [{\"action_type\": \"ATTACK_ENEMY\", \"parameters\": {}, \"priority\": 1.0}]\n",
    "            },\n",
    "            {\n",
    "                \"subgoal\": \"Conservative approach\",\n",
    "                \"reasoning\": \"Cautious movement to preserve health\",\n",
    "                \"macros\": [{\"action_type\": \"MOVE_TO\", \"parameters\": {\"x\": 2, \"y\": 2}, \"priority\": 0.5}]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def generate_plan_variations(self, base_state, num_variations=4):\n",
    "        \"\"\"Generate plan variations based on game state.\"\"\"\n",
    "        variations = []\n",
    "        \n",
    "        # Health-based variations\n",
    "        player = base_state.get('player', {})\n",
    "        health_ratio = player.get('health', 3) / max(player.get('max_health', 3), 1)\n",
    "        \n",
    "        if health_ratio < 0.5:\n",
    "            # Low health - conservative plans\n",
    "            variations.extend(self.plan_templates[3:4] * 2)\n",
    "        else:\n",
    "            # Good health - aggressive plans\n",
    "            variations.extend(self.plan_templates[:3])\n",
    "        \n",
    "        # Add some random variations\n",
    "        while len(variations) < num_variations:\n",
    "            variations.append(np.random.choice(self.plan_templates))\n",
    "        \n",
    "        return variations[:num_variations]\n",
    "\n",
    "plan_variator = PlanVariator()\n",
    "print(\"Plan variator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def collect_rollout_with_plan(env, agent, plan, max_steps=100):\n",
    "    \"\"\"Collect rollout with specific plan.\"\"\"\n",
    "    if not env or not agent:\n",
    "        return None\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    initial_state = info.get('structured_state', {})\n",
    "    \n",
    "    # Extract initial metrics\n",
    "    initial_rupees = initial_state.get('resources', {}).get('rupees', 0)\n",
    "    initial_health = initial_state.get('player', {}).get('health', 3)\n",
    "    initial_pos = (initial_state.get('player', {}).get('x', 0), \n",
    "                   initial_state.get('player', {}).get('y', 0))\n",
    "    \n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    positions_visited = set()\n",
    "    \n",
    "    # Override agent's planner with our specific plan\n",
    "    # (This is simplified - in practice you'd inject the plan into the macro executor)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        structured_state = env.get_structured_state()\n",
    "        \n",
    "        # Track position for exploration metric\n",
    "        player_pos = (structured_state.get('player', {}).get('x', 0),\n",
    "                     structured_state.get('player', {}).get('y', 0))\n",
    "        positions_visited.add(player_pos)\n",
    "        \n",
    "        # Get action from agent\n",
    "        action = await agent.act(obs, structured_state)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Extract final metrics\n",
    "    final_state = info.get('structured_state', {})\n",
    "    final_rupees = final_state.get('resources', {}).get('rupees', 0)\n",
    "    final_health = final_state.get('player', {}).get('health', 3)\n",
    "    final_pos = (final_state.get('player', {}).get('x', 0),\n",
    "                 final_state.get('player', {}).get('y', 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rupees_gained = final_rupees - initial_rupees\n",
    "    health_change = final_health - initial_health\n",
    "    distance_traveled = abs(final_pos[0] - initial_pos[0]) + abs(final_pos[1] - initial_pos[1])\n",
    "    exploration_score = len(positions_visited)\n",
    "    \n",
    "    return {\n",
    "        'plan': plan,\n",
    "        'initial_state': initial_state,\n",
    "        'final_state': final_state,\n",
    "        'total_reward': total_reward,\n",
    "        'step_count': step_count,\n",
    "        'rupees_gained': rupees_gained,\n",
    "        'health_change': health_change,\n",
    "        'distance_traveled': distance_traveled,\n",
    "        'exploration_score': exploration_score,\n",
    "        'terminated': terminated\n",
    "    }\n",
    "\n",
    "print(\"Rollout collection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Data Collection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def collect_preference_batch():\n",
    "    \"\"\"Collect a batch of rollouts with different plans.\"\"\"\n",
    "    if not env or not agent:\n",
    "        print(\"‚ùå Cannot collect data - components not initialized\")\n",
    "        return []\n",
    "    \n",
    "    # Get initial state for plan generation\n",
    "    obs, info = env.reset()\n",
    "    base_state = info.get('structured_state', {})\n",
    "    \n",
    "    # Generate plan variations\n",
    "    plans = plan_variator.generate_plan_variations(base_state, CONFIG['rollouts_per_batch'])\n",
    "    \n",
    "    rollout_results = []\n",
    "    \n",
    "    for i, plan in enumerate(plans):\n",
    "        print(f\"  Collecting rollout {i+1}/{len(plans)}: {plan['subgoal']}\")\n",
    "        \n",
    "        result = await collect_rollout_with_plan(env, agent, plan, CONFIG['rollout_length'])\n",
    "        if result:\n",
    "            rollout_results.append(result)\n",
    "    \n",
    "    return rollout_results\n",
    "\n",
    "# Data collection and analysis\n",
    "class GRPOAnalyzer:\n",
    "    \"\"\"Analyze GRPO training data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.all_rollouts = []\n",
    "        self.preference_pairs = []\n",
    "    \n",
    "    def add_batch(self, rollouts):\n",
    "        \"\"\"Add rollout batch and create preference pairs.\"\"\"\n",
    "        self.all_rollouts.extend(rollouts)\n",
    "        \n",
    "        # Create preference pairs from this batch\n",
    "        for i in range(len(rollouts)):\n",
    "            for j in range(i + 1, len(rollouts)):\n",
    "                r1, r2 = rollouts[i], rollouts[j]\n",
    "                reward_diff = abs(r1['total_reward'] - r2['total_reward'])\n",
    "                \n",
    "                if reward_diff > CONFIG['preference_threshold']:\n",
    "                    if r1['total_reward'] > r2['total_reward']:\n",
    "                        preferred, dispreferred = r1, r2\n",
    "                    else:\n",
    "                        preferred, dispreferred = r2, r1\n",
    "                    \n",
    "                    self.preference_pairs.append({\n",
    "                        'preferred': preferred,\n",
    "                        'dispreferred': dispreferred,\n",
    "                        'reward_diff': reward_diff\n",
    "                    })\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"Analyze collected data.\"\"\"\n",
    "        if not self.all_rollouts:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.all_rollouts)\n",
    "        \n",
    "        analysis = {\n",
    "            'total_rollouts': len(self.all_rollouts),\n",
    "            'preference_pairs': len(self.preference_pairs),\n",
    "            'avg_reward': df['total_reward'].mean(),\n",
    "            'avg_rupees_gained': df['rupees_gained'].mean(),\n",
    "            'avg_exploration': df['exploration_score'].mean(),\n",
    "            'termination_rate': df['terminated'].mean(),\n",
    "            'plan_performance': df.groupby(df['plan'].apply(lambda x: x['subgoal']))['total_reward'].agg(['mean', 'std', 'count'])\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def plot_analysis(self):\n",
    "        \"\"\"Plot analysis results.\"\"\"\n",
    "        if not self.all_rollouts:\n",
    "            print(\"No data to plot\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.all_rollouts)\n",
    "        df['plan_type'] = df['plan'].apply(lambda x: x['subgoal'])\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Reward by plan type\n",
    "        sns.boxplot(data=df, x='plan_type', y='total_reward', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Reward by Plan Type')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Exploration vs Reward\n",
    "        sns.scatterplot(data=df, x='exploration_score', y='total_reward', \n",
    "                       hue='plan_type', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Exploration vs Reward')\n",
    "        \n",
    "        # Rupees gained by plan type\n",
    "        sns.boxplot(data=df, x='plan_type', y='rupees_gained', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Rupees Gained by Plan Type')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Reward distribution\n",
    "        axes[1,1].hist(df['total_reward'], bins=20, alpha=0.7)\n",
    "        axes[1,1].set_title('Reward Distribution')\n",
    "        axes[1,1].set_xlabel('Total Reward')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "analyzer = GRPOAnalyzer()\n",
    "print(\"GRPO analyzer created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main GRPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def run_grpo_experiment():\n",
    "    \"\"\"Run GRPO experiment with data collection and analysis.\"\"\"\n",
    "    if not env or not agent:\n",
    "        print(\"‚ùå Cannot run experiment - components not initialized\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üöÄ Starting GRPO experiment for {CONFIG['total_episodes']} episodes\")\n",
    "    print(f\"Rollouts per batch: {CONFIG['rollouts_per_batch']}\")\n",
    "    print(f\"Rollout length: {CONFIG['rollout_length']}\")\n",
    "    \n",
    "    episodes_completed = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    while episodes_completed < CONFIG['total_episodes']:\n",
    "        batch_count += 1\n",
    "        \n",
    "        print(f\"\\nüìä Collecting batch {batch_count} (episodes {episodes_completed+1}-{episodes_completed+CONFIG['rollouts_per_batch']})\")\n",
    "        \n",
    "        # Collect preference batch\n",
    "        rollouts = await collect_preference_batch()\n",
    "        \n",
    "        if rollouts:\n",
    "            # Add to analyzer\n",
    "            analyzer.add_batch(rollouts)\n",
    "            \n",
    "            # Print batch results\n",
    "            print(f\"Batch {batch_count} results:\")\n",
    "            for i, rollout in enumerate(rollouts):\n",
    "                print(f\"  {i+1}. {rollout['plan']['subgoal']}: \"\n",
    "                      f\"Reward={rollout['total_reward']:.3f}, \"\n",
    "                      f\"Rupees={rollout['rupees_gained']}, \"\n",
    "                      f\"Exploration={rollout['exploration_score']}\")\n",
    "            \n",
    "            episodes_completed += len(rollouts)\n",
    "            \n",
    "            # Periodic analysis\n",
    "            if batch_count % 3 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"üìà Analysis after {episodes_completed} episodes:\")\n",
    "                \n",
    "                analysis = analyzer.analyze()\n",
    "                print(f\"Total rollouts: {analysis['total_rollouts']}\")\n",
    "                print(f\"Preference pairs: {analysis['preference_pairs']}\")\n",
    "                print(f\"Average reward: {analysis['avg_reward']:.3f}\")\n",
    "                print(f\"Average rupees gained: {analysis['avg_rupees_gained']:.1f}\")\n",
    "                print(f\"Average exploration score: {analysis['avg_exploration']:.1f}\")\n",
    "                print(f\"Termination rate: {analysis['termination_rate']:.1%}\")\n",
    "                \n",
    "                print(\"\\nPlan performance:\")\n",
    "                display(analysis['plan_performance'])\n",
    "                \n",
    "                # Plot analysis\n",
    "                analyzer.plot_analysis()\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to collect rollouts for batch {batch_count}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüéâ GRPO experiment completed!\")\n",
    "    print(f\"Total episodes: {episodes_completed}\")\n",
    "    print(f\"Total batches: {batch_count}\")\n",
    "    \n",
    "    # Final analysis\n",
    "    final_analysis = analyzer.analyze()\n",
    "    print(f\"\\nüìä Final Analysis:\")\n",
    "    print(f\"Preference pairs created: {final_analysis['preference_pairs']}\")\n",
    "    print(f\"Best performing plan type: {final_analysis['plan_performance']['mean'].idxmax()}\")\n",
    "    \n",
    "    # Final plots\n",
    "    analyzer.plot_analysis()\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Run the experiment\n",
    "final_analyzer = await run_grpo_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_preferences(analyzer):\n",
    "    \"\"\"Analyze preference patterns.\"\"\"\n",
    "    if not analyzer.preference_pairs:\n",
    "        print(\"No preference pairs to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìã Preference Analysis ({len(analyzer.preference_pairs)} pairs):\")\n",
    "    \n",
    "    # Analyze preferred plan types\n",
    "    preferred_plans = [pair['preferred']['plan']['subgoal'] for pair in analyzer.preference_pairs]\n",
    "    dispreferred_plans = [pair['dispreferred']['plan']['subgoal'] for pair in analyzer.preference_pairs]\n",
    "    \n",
    "    from collections import Counter\n",
    "    preferred_counts = Counter(preferred_plans)\n",
    "    dispreferred_counts = Counter(dispreferred_plans)\n",
    "    \n",
    "    print(\"\\nMost preferred plan types:\")\n",
    "    for plan_type, count in preferred_counts.most_common():\n",
    "        print(f\"  {plan_type}: {count} times preferred\")\n",
    "    \n",
    "    print(\"\\nMost dispreferred plan types:\")\n",
    "    for plan_type, count in dispreferred_counts.most_common():\n",
    "        print(f\"  {plan_type}: {count} times dispreferred\")\n",
    "    \n",
    "    # Analyze preference strength\n",
    "    reward_diffs = [pair['reward_diff'] for pair in analyzer.preference_pairs]\n",
    "    print(f\"\\nPreference strength:\")\n",
    "    print(f\"  Average reward difference: {np.mean(reward_diffs):.3f}\")\n",
    "    print(f\"  Max reward difference: {max(reward_diffs):.3f}\")\n",
    "    print(f\"  Min reward difference: {min(reward_diffs):.3f}\")\n",
    "    \n",
    "    # Plot preference analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Preferred vs dispreferred plan types\n",
    "    plan_types = list(set(preferred_plans + dispreferred_plans))\n",
    "    preferred_vals = [preferred_counts[pt] for pt in plan_types]\n",
    "    dispreferred_vals = [dispreferred_counts[pt] for pt in plan_types]\n",
    "    \n",
    "    x = np.arange(len(plan_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, preferred_vals, width, label='Preferred', alpha=0.8)\n",
    "    axes[0].bar(x + width/2, dispreferred_vals, width, label='Dispreferred', alpha=0.8)\n",
    "    axes[0].set_xlabel('Plan Type')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Preferred vs Dispreferred Plans')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels([pt[:10] + '...' if len(pt) > 10 else pt for pt in plan_types], rotation=45)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Reward difference distribution\n",
    "    axes[1].hist(reward_diffs, bins=10, alpha=0.7)\n",
    "    axes[1].set_xlabel('Reward Difference')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Preference Strength Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if final_analyzer:\n",
    "    analyze_preferences(final_analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan Quality Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plan_insights(analyzer):\n",
    "    \"\"\"Extract insights about plan quality.\"\"\"\n",
    "    if not analyzer.all_rollouts:\n",
    "        print(\"No rollout data available\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(analyzer.all_rollouts)\n",
    "    df['plan_type'] = df['plan'].apply(lambda x: x['subgoal'])\n",
    "    \n",
    "    print(\"üîç Plan Quality Insights:\")\n",
    "    \n",
    "    # Performance by plan type\n",
    "    performance = df.groupby('plan_type').agg({\n",
    "        'total_reward': ['mean', 'std', 'count'],\n",
    "        'rupees_gained': 'mean',\n",
    "        'exploration_score': 'mean',\n",
    "        'terminated': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"\\nPerformance by plan type:\")\n",
    "    display(performance)\n",
    "    \n",
    "    # Best and worst plans\n",
    "    best_plan_idx = df['total_reward'].idxmax()\n",
    "    worst_plan_idx = df['total_reward'].idxmin()\n",
    "    \n",
    "    best_plan = df.loc[best_plan_idx]\n",
    "    worst_plan = df.loc[worst_plan_idx]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best performing plan:\")\n",
    "    print(f\"  Type: {best_plan['plan_type']}\")\n",
    "    print(f\"  Reward: {best_plan['total_reward']:.3f}\")\n",
    "    print(f\"  Reasoning: {best_plan['plan']['reasoning']}\")\n",
    "    \n",
    "    print(f\"\\nüí• Worst performing plan:\")\n",
    "    print(f\"  Type: {worst_plan['plan_type']}\")\n",
    "    print(f\"  Reward: {worst_plan['total_reward']:.3f}\")\n",
    "    print(f\"  Reasoning: {worst_plan['plan']['reasoning']}\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlations = df[['total_reward', 'rupees_gained', 'exploration_score', 'distance_traveled']].corr()\n",
    "    \n",
    "    print(\"\\nüìà Metric Correlations:\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlations, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix of Performance Metrics')\n",
    "    plt.show()\n",
    "    \n",
    "    # Key insights\n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    \n",
    "    # Exploration vs reward correlation\n",
    "    explore_reward_corr = correlations.loc['total_reward', 'exploration_score']\n",
    "    if explore_reward_corr > 0.3:\n",
    "        print(f\"  ‚úÖ Exploration strongly correlates with reward (r={explore_reward_corr:.2f})\")\n",
    "    elif explore_reward_corr > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è Exploration moderately correlates with reward (r={explore_reward_corr:.2f})\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Exploration weakly correlates with reward (r={explore_reward_corr:.2f})\")\n",
    "    \n",
    "    # Best strategy\n",
    "    best_strategy = performance['total_reward']['mean'].idxmax()\n",
    "    print(f\"  üéØ Best overall strategy: {best_strategy}\")\n",
    "    \n",
    "    # Consistency\n",
    "    most_consistent = performance['total_reward']['std'].idxmin()\n",
    "    print(f\"  üé≤ Most consistent strategy: {most_consistent}\")\n",
    "\n",
    "if final_analyzer:\n",
    "    extract_plan_insights(final_analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['save_data'] and final_analyzer:\n",
    "    # Create output directory\n",
    "    output_dir = project_root / 'data' / 'grpo_results'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save rollout data\n",
    "    rollout_file = output_dir / 'rollout_data.json'\n",
    "    with open(rollout_file, 'w') as f:\n",
    "        json.dump(final_analyzer.all_rollouts, f, indent=2, default=str)\n",
    "    \n",
    "    # Save preference pairs\n",
    "    preference_file = output_dir / 'preference_pairs.json'\n",
    "    with open(preference_file, 'w') as f:\n",
    "        json.dump(final_analyzer.preference_pairs, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Data saved to {output_dir}\")\n",
    "    print(f\"  - Rollout data: {rollout_file}\")\n",
    "    print(f\"  - Preference pairs: {preference_file}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping data save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "if env:\n",
    "    env.close()\n",
    "    print(\"Environment closed\")\n",
    "\n",
    "if agent:\n",
    "    await agent.close()\n",
    "    print(\"Agent closed\")\n",
    "\n",
    "if planner:\n",
    "    await planner.close()\n",
    "    print(\"Planner closed\")\n",
    "\n",
    "print(\"üßπ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Plan Variation Generation**: Created different strategic plans for the same game state\n",
    "2. **Preference Data Collection**: Collected rollouts with different plans and compared outcomes\n",
    "3. **Preference Analysis**: Identified which types of plans lead to better performance\n",
    "4. **Quality Insights**: Analyzed correlations between planning strategies and game metrics\n",
    "\n",
    "### Key Findings:\n",
    "- Different planning strategies show measurable performance differences\n",
    "- Exploration-focused plans often correlate with better rewards\n",
    "- Some strategies are more consistent than others\n",
    "- Preference pairs can be used to train a better planner\n",
    "\n",
    "### Next Steps:\n",
    "- Use the collected preference data to train a real LLM planner\n",
    "- Deploy the improved planner on OpenShift using the KServe manifests\n",
    "- Run full-scale training with the complete system\n",
    "- Experiment with more sophisticated plan generation strategies\n",
    "\n",
    "This GRPO approach provides a foundation for improving the LLM planner through preference optimization, leading to better strategic decision-making in the Zelda environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}