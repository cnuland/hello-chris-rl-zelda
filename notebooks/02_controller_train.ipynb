{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zelda PPO Controller Training\n",
    "\n",
    "This notebook demonstrates PPO training for the Zelda Oracle of Seasons controller.\n",
    "It provides an interactive training loop with live monitoring and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path('../').resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'rom_path': '../roms/zelda_oracle_of_seasons.gbc',  # Update with your ROM path\n",
    "    'total_timesteps': 100000,  # Reduced for notebook demo\n",
    "    'rollout_length': 64,       # Steps per rollout\n",
    "    'update_frequency': 1000,   # Update every N steps\n",
    "    'eval_frequency': 5000,     # Evaluate every N steps\n",
    "    'log_frequency': 500,       # Log every N steps\n",
    "    'save_frequency': 10000,    # Save checkpoint every N steps\n",
    "    'use_mock_planner': True,   # Use mock planner for demo\n",
    "    'planner_frequency': 50,    # Call planner every N steps\n",
    "}\n",
    "\n",
    "# Check ROM path\n",
    "rom_path = Path(CONFIG['rom_path'])\n",
    "if not rom_path.exists():\n",
    "    # Try to find any ROM file\n",
    "    rom_dir = project_root / 'roms'\n",
    "    rom_files = list(rom_dir.glob('*.gbc')) + list(rom_dir.glob('*.gb'))\n",
    "    if rom_files:\n",
    "        CONFIG['rom_path'] = str(rom_files[0])\n",
    "        print(f\"Using ROM: {CONFIG['rom_path']}\")\n",
    "    else:\n",
    "        print(\"‚ùå No ROM file found! Please add a ROM to the roms/ directory.\")\n",
    "        CONFIG['rom_path'] = None\n",
    "\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['rom_path']:\n",
    "    from emulator.zelda_env import ZeldaEnvironment\n",
    "    from agents.controller import HybridAgent, ControllerConfig\n",
    "    \n",
    "    # Create environment\n",
    "    env = ZeldaEnvironment(CONFIG['rom_path'], headless=True)\n",
    "    \n",
    "    # Create agent configuration\n",
    "    agent_config = ControllerConfig(\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        use_planner=CONFIG['use_mock_planner'],\n",
    "        planner_frequency=CONFIG['planner_frequency']\n",
    "    )\n",
    "    \n",
    "    # Create hybrid agent\n",
    "    agent = HybridAgent(env, agent_config, use_mock_planner=CONFIG['use_mock_planner'])\n",
    "    \n",
    "    print(f\"Environment: {env}\")\n",
    "    print(f\"Agent device: {agent.controller.device}\")\n",
    "    print(f\"Observation space: {env.observation_space}\")\n",
    "    print(f\"Action space: {env.action_space}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping initialization - no ROM file available\")\n",
    "    env = None\n",
    "    agent = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"Monitor training progress with live plotting.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.metrics = {\n",
    "            'episode_rewards': deque(maxlen=window_size),\n",
    "            'episode_lengths': deque(maxlen=window_size),\n",
    "            'policy_losses': deque(maxlen=window_size),\n",
    "            'value_losses': deque(maxlen=window_size),\n",
    "            'rupees_collected': deque(maxlen=window_size),\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "    def update(self, step, **kwargs):\n",
    "        \"\"\"Update metrics.\"\"\"\n",
    "        self.metrics['steps'].append(step)\n",
    "        for key, value in kwargs.items():\n",
    "            if key in self.metrics:\n",
    "                self.metrics[key].append(value)\n",
    "    \n",
    "    def plot(self, figsize=(15, 10)):\n",
    "        \"\"\"Plot training metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Episode rewards\n",
    "        if self.metrics['episode_rewards']:\n",
    "            axes[0].plot(self.metrics['episode_rewards'])\n",
    "            axes[0].set_title('Episode Rewards')\n",
    "            axes[0].set_ylabel('Reward')\n",
    "            axes[0].grid(True)\n",
    "        \n",
    "        # Episode lengths\n",
    "        if self.metrics['episode_lengths']:\n",
    "            axes[1].plot(self.metrics['episode_lengths'])\n",
    "            axes[1].set_title('Episode Lengths')\n",
    "            axes[1].set_ylabel('Steps')\n",
    "            axes[1].grid(True)\n",
    "        \n",
    "        # Policy loss\n",
    "        if self.metrics['policy_losses']:\n",
    "            axes[2].plot(self.metrics['policy_losses'])\n",
    "            axes[2].set_title('Policy Loss')\n",
    "            axes[2].set_ylabel('Loss')\n",
    "            axes[2].grid(True)\n",
    "        \n",
    "        # Value loss\n",
    "        if self.metrics['value_losses']:\n",
    "            axes[3].plot(self.metrics['value_losses'])\n",
    "            axes[3].set_title('Value Loss')\n",
    "            axes[3].set_ylabel('Loss')\n",
    "            axes[3].grid(True)\n",
    "        \n",
    "        # Rupees collected\n",
    "        if self.metrics['rupees_collected']:\n",
    "            axes[4].plot(self.metrics['rupees_collected'])\n",
    "            axes[4].set_title('Rupees Collected')\n",
    "            axes[4].set_ylabel('Rupees')\n",
    "            axes[4].grid(True)\n",
    "        \n",
    "        # Summary stats\n",
    "        axes[5].text(0.1, 0.8, f\"Total Episodes: {len(self.metrics['episode_rewards'])}\")\n",
    "        if self.metrics['episode_rewards']:\n",
    "            avg_reward = np.mean(list(self.metrics['episode_rewards'])[-20:])\n",
    "            axes[5].text(0.1, 0.6, f\"Avg Reward (last 20): {avg_reward:.2f}\")\n",
    "        if self.metrics['episode_lengths']:\n",
    "            avg_length = np.mean(list(self.metrics['episode_lengths'])[-20:])\n",
    "            axes[5].text(0.1, 0.4, f\"Avg Length (last 20): {avg_length:.1f}\")\n",
    "        axes[5].set_title('Summary')\n",
    "        axes[5].set_xlim(0, 1)\n",
    "        axes[5].set_ylim(0, 1)\n",
    "        axes[5].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create monitor\n",
    "monitor = TrainingMonitor()\n",
    "print(\"Training monitor created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def collect_rollout(env, agent, max_steps=64):\n",
    "    \"\"\"Collect a rollout of experience.\"\"\"\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    dones = []\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    initial_rupees = info.get('structured_state', {}).get('resources', {}).get('rupees', 0)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        observations.append(obs.copy())\n",
    "        \n",
    "        # Get action from agent\n",
    "        structured_state = env.get_structured_state()\n",
    "        action = await agent.act(obs, structured_state)\n",
    "        \n",
    "        # Get policy outputs for training\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(agent.controller.device)\n",
    "        with torch.no_grad():\n",
    "            action_tensor, log_prob, value = agent.controller.policy_net.get_action_and_value(obs_tensor)\n",
    "        \n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob.item())\n",
    "        values.append(value.item())\n",
    "        \n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        done = terminated or truncated\n",
    "        dones.append(done)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate episode metrics\n",
    "    final_rupees = info.get('structured_state', {}).get('resources', {}).get('rupees', 0)\n",
    "    rupees_collected = final_rupees - initial_rupees\n",
    "    \n",
    "    return {\n",
    "        'observations': observations,\n",
    "        'actions': actions,\n",
    "        'log_probs': log_probs,\n",
    "        'rewards': rewards,\n",
    "        'values': values,\n",
    "        'dones': dones,\n",
    "        'episode_reward': sum(rewards),\n",
    "        'episode_length': len(rewards),\n",
    "        'rupees_collected': rupees_collected\n",
    "    }\n",
    "\n",
    "async def training_loop():\n",
    "    \"\"\"Main training loop.\"\"\"\n",
    "    if not env or not agent:\n",
    "        print(\"‚ùå Cannot start training - environment or agent not initialized\")\n",
    "        return\n",
    "    \n",
    "    print(\"üöÄ Starting training loop...\")\n",
    "    \n",
    "    global_step = 0\n",
    "    episode_count = 0\n",
    "    \n",
    "    while global_step < CONFIG['total_timesteps']:\n",
    "        # Collect rollout\n",
    "        rollout_data = await collect_rollout(env, agent, CONFIG['rollout_length'])\n",
    "        \n",
    "        global_step += rollout_data['episode_length']\n",
    "        episode_count += 1\n",
    "        \n",
    "        # Train agent\n",
    "        if len(rollout_data['observations']) > 0:\n",
    "            # Convert to tensors\n",
    "            observations = torch.FloatTensor(rollout_data['observations']).to(agent.controller.device)\n",
    "            actions = torch.LongTensor(rollout_data['actions']).to(agent.controller.device)\n",
    "            old_log_probs = torch.FloatTensor(rollout_data['log_probs']).to(agent.controller.device)\n",
    "            \n",
    "            # Compute advantages and returns\n",
    "            advantages, returns = agent.controller.compute_gae(\n",
    "                rollout_data['rewards'], \n",
    "                rollout_data['values'], \n",
    "                rollout_data['dones']\n",
    "            )\n",
    "            advantages = torch.FloatTensor(advantages).to(agent.controller.device)\n",
    "            returns = torch.FloatTensor(returns).to(agent.controller.device)\n",
    "            \n",
    "            # Prepare batch data\n",
    "            batch_data = {\n",
    "                'obs': observations,\n",
    "                'actions': actions,\n",
    "                'log_probs': old_log_probs,\n",
    "                'advantages': advantages,\n",
    "                'returns': returns\n",
    "            }\n",
    "            \n",
    "            # Update policy\n",
    "            training_metrics = agent.controller.update(batch_data, epochs=2)\n",
    "        else:\n",
    "            training_metrics = {'policy_loss': 0, 'value_loss': 0}\n",
    "        \n",
    "        # Update monitor\n",
    "        monitor.update(\n",
    "            step=global_step,\n",
    "            episode_rewards=rollout_data['episode_reward'],\n",
    "            episode_lengths=rollout_data['episode_length'],\n",
    "            policy_losses=training_metrics['policy_loss'],\n",
    "            value_losses=training_metrics['value_loss'],\n",
    "            rupees_collected=rollout_data['rupees_collected']\n",
    "        )\n",
    "        \n",
    "        # Logging\n",
    "        if global_step % CONFIG['log_frequency'] == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Step {global_step}/{CONFIG['total_timesteps']} | Episode {episode_count}\")\n",
    "            print(f\"Reward: {rollout_data['episode_reward']:.3f} | Length: {rollout_data['episode_length']}\")\n",
    "            print(f\"Policy Loss: {training_metrics['policy_loss']:.4f} | Value Loss: {training_metrics['value_loss']:.4f}\")\n",
    "            print(f\"Rupees: {rollout_data['rupees_collected']} | Agent metrics: {agent.get_metrics()}\")\n",
    "            \n",
    "            # Plot metrics\n",
    "            monitor.plot(figsize=(12, 8))\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if global_step % CONFIG['save_frequency'] == 0:\n",
    "            checkpoint_path = f\"../checkpoints/notebook_checkpoint_{global_step}.pt\"\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            agent.controller.save_checkpoint(checkpoint_path)\n",
    "            print(f\"üíæ Saved checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    print(\"üéâ Training completed!\")\n",
    "    \n",
    "    # Final plot\n",
    "    monitor.plot(figsize=(15, 10))\n",
    "\n",
    "# Run training loop\n",
    "await training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_agent(num_episodes=5):\n",
    "    \"\"\"Evaluate the trained agent.\"\"\"\n",
    "    if not env or not agent:\n",
    "        print(\"‚ùå Cannot evaluate - environment or agent not initialized\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Evaluating agent for {num_episodes} episodes...\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    episode_metrics = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        \n",
    "        initial_state = info.get('structured_state', {})\n",
    "        initial_rupees = initial_state.get('resources', {}).get('rupees', 0)\n",
    "        initial_health = initial_state.get('player', {}).get('health', 0)\n",
    "        \n",
    "        while True:\n",
    "            # Use deterministic policy for evaluation\n",
    "            action = agent.controller.act_deterministic(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            if terminated or truncated or episode_length >= 1000:\n",
    "                break\n",
    "        \n",
    "        final_state = info.get('structured_state', {})\n",
    "        final_rupees = final_state.get('resources', {}).get('rupees', 0)\n",
    "        final_health = final_state.get('player', {}).get('health', 0)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_metrics.append({\n",
    "            'rupees_gained': final_rupees - initial_rupees,\n",
    "            'health_change': final_health - initial_health,\n",
    "            'terminated': terminated\n",
    "        })\n",
    "        \n",
    "        print(f\"Episode {episode+1}: Reward={episode_reward:.3f}, Length={episode_length}, \"\n",
    "              f\"Rupees={final_rupees-initial_rupees}, Health={final_health}/{initial_health}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"Mean reward: {np.mean(episode_rewards):.3f} ¬± {np.std(episode_rewards):.3f}\")\n",
    "    print(f\"Mean length: {np.mean(episode_lengths):.1f} ¬± {np.std(episode_lengths):.1f}\")\n",
    "    print(f\"Mean rupees gained: {np.mean([m['rupees_gained'] for m in episode_metrics]):.1f}\")\n",
    "    print(f\"Termination rate: {np.mean([m['terminated'] for m in episode_metrics]):.2%}\")\n",
    "    \n",
    "    # Plot evaluation results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].bar(range(len(episode_rewards)), episode_rewards)\n",
    "    axes[0].set_title('Episode Rewards')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Reward')\n",
    "    \n",
    "    axes[1].bar(range(len(episode_lengths)), episode_lengths)\n",
    "    axes[1].set_title('Episode Lengths')\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'metrics': episode_metrics\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = await evaluate_agent(num_episodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "if env:\n",
    "    env.close()\n",
    "    print(\"Environment closed\")\n",
    "\n",
    "if agent:\n",
    "    await agent.close()\n",
    "    print(\"Agent closed\")\n",
    "\n",
    "print(\"üßπ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Interactive PPO Training**: Real-time monitoring with live plots\n",
    "2. **Hybrid Agent**: Integration of PPO controller with mock LLM planner\n",
    "3. **Game-specific Metrics**: Tracking rupees, health, exploration\n",
    "4. **Evaluation**: Deterministic policy evaluation\n",
    "\n",
    "### Key Observations:\n",
    "- The agent learns basic movement and interaction\n",
    "- Mock planner provides strategic guidance\n",
    "- Reward shaping encourages exploration and resource collection\n",
    "\n",
    "### Next Steps:\n",
    "- Use `03_planner_grpo.ipynb` for LLM planner optimization\n",
    "- Run longer training with `training/run_cleanrl.py`\n",
    "- Deploy LLM planner on OpenShift for full system integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}